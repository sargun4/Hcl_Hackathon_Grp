{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56346ac",
   "metadata": {},
   "source": [
    "## Setting up the Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d7d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCq9RkCd1Jw8dLUdSLQA2sbdL2d8Wmou1Q\"\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cb73a",
   "metadata": {},
   "source": [
    "## Setting up the HuggingFace Embeddings Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "696bb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74758296",
   "metadata": {},
   "source": [
    "## Setting up FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e48bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2f26a",
   "metadata": {},
   "source": [
    "## TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3937d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d8d1f",
   "metadata": {},
   "source": [
    "## Processing all the docs in the folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5f6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 233 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 documents from 3 PDF files.\n",
      "Split 60 PDF docs into 252 chunks.\n",
      "Uploaded 252 PDF chunks to FAISS. Sample ids: ['2e4831d0-3e08-487f-b83c-35aca832038a', 'aa5176dc-ab27-4f3c-a5b9-078d5ad1ef23', '63baf6b1-b5dc-4ae7-9e1a-65c358ea9890']\n"
     ]
    }
   ],
   "source": [
    "# load all PDF files from the ./docs folder using the existing PyPDFLoader\n",
    "pdf_files = [\n",
    "    os.path.join(\"docs\", f)\n",
    "    for f in os.listdir(\"docs\")\n",
    "    if os.path.isfile(os.path.join(\"docs\", f)) and f.lower().endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "all_pdf_docs = []\n",
    "for path in pdf_files:\n",
    "    loader = PyPDFLoader(path)\n",
    "    loaded = loader.load()\n",
    "    # ensure each document records its source file\n",
    "    for d in loaded:\n",
    "        d.metadata[\"source\"] = path\n",
    "    all_pdf_docs.extend(loaded)\n",
    "\n",
    "print(f\"Loaded {len(all_pdf_docs)} documents from {len(pdf_files)} PDF files.\")\n",
    "pdf_files[:10]\n",
    "\n",
    "# --- chunk, embed, and upload PDFs to existing FAISS vector_store ---\n",
    "# reuse the existing text_splitter defined in a later cell\n",
    "pdf_splits = text_splitter.split_documents(all_pdf_docs)\n",
    "print(f\"Split {len(all_pdf_docs)} PDF docs into {len(pdf_splits)} chunks.\")\n",
    "\n",
    "# add to the existing vector_store (uses the embeddings already configured)\n",
    "pdf_document_ids = vector_store.add_documents(documents=pdf_splits)\n",
    "print(f\"Uploaded {len(pdf_document_ids)} PDF chunks to FAISS. Sample ids: {pdf_document_ids[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de6f3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='SMPL: A Skinned Multi-Person Linear Model\\nMatthew Loper⇤12 Naureen Mahmood†1 Javier Romero†1 Gerard Pons-Moll†1 Michael J. Black†1\\n1Max Planck Institute for Intelligent Systems, T¨ubingen, Germany\\n2Industrial Light and Magic, San Francisco, CA\\nFigure 1: SMPL is a realistic learned model of human body shape and pose that is compatible with existing rendering engines, allows\\nanimator control, and is available for research purposes. (left) SMPL model (orange) ﬁt to ground truth 3D meshes (gray). (right) Unity 5.0\\ngame engine screenshot showing bodies from the CAESAR dataset animated in real time.\\nAbstract\\nWe present a learned model of human body shape and pose-\\ndependent shape variation that is more accurate than previous\\nmodels and is compatible with existing graphics pipelines. Our\\nSkinned Multi-Person Linear model (SMPL) is a skinned vertex-\\nbased model that accurately represents a wide variety of body\\nshapes in natural human poses. The parameters of the model are\\nlearned from data including the rest pose template, blend weights,\\npose-dependent blend shapes, identity-dependent blend shapes, and\\na regressor from vertices to joint locations. Unlike previous mod-\\nels, the pose-dependent blend shapes are a linear function of the\\nelements of the pose rotation matrices. This simple formulation en-\\nables training the entire model from a relatively large number of\\naligned 3D meshes of different people in different poses. We quan-\\ntitatively evaluate variants of SMPL using linear or dual-quaternion\\nblend skinning and show that both are more accurate than a Blend-\\nSCAPE model trained on the same data. We also extend SMPL to\\nrealistically model dynamic soft-tissue deformations. Because it is\\nbased on blend skinning, SMPL is compatible with existing render-\\ning engines and we make it available for research purposes.\\nCR Categories: I.3.3 [Computer Graphics]: Three-Dimensional\\nGraphics and Realism—Animation\\nKeywords: Body shape, skinning, blendshapes, soft-tissue.\\n⇤e-mail:mloper@ilm.com\\n†e-mail:{nmahmood,jromero,gerard.pons.moll,black}@tue.mpg.de\\n1 Introduction\\nOur goal is to create realistic animated human bodies that can rep-\\nresent different body shapes, deform naturally with pose, and ex-\\nhibit soft-tissue motions like those of real humans. We want such\\nmodels to be fast to render, easy to deploy, and compatible with\\nexisting rendering engines. The commercial approach commonly\\ninvolves hand rigging a mesh and manually sculpting blend shapes\\nto correct problems with traditional skinning methods. Many blend\\nshapes are typically needed and the manual effort required to build\\nthem is large. As an alternative, the research community has fo-\\ncused on learning statistical body models from example scans of\\ndifferent bodies in a varied set of poses. While promising, these\\napproaches are not compatible with existing graphics software and\\nrendering engines that use standard skinning methods.\\nOur goal is to automatically learn a model of the body that is both\\nrealistic and compatible with existing graphics software. To that\\nend, we describe a “Skinned Multi-Person Linear” (SMPL) model\\nof the human body that can realistically represent a wide range of\\nhuman body shapes, can be posed with natural pose-dependent de-\\nformations, exhibits soft-tissue dynamics, is efﬁcient to animate,\\nand is compatible with existing rendering engines (Fig. 1).\\nTraditional methods model how vertices are related to an underly-\\ning skeleton structure. Basic linear blend skinning (LBS) models\\nare the most widely used, are supported by all game engines, and\\nare efﬁcient to render. Unfortunately they produce unrealistic de-\\nformations at joints including the well-known “taffy” and “bowtie”\\neffects (see Fig. 2). Tremendous work has gone into skinning meth-\\nods that ameliorate these effects [Lewis et al. 2000; Wang and\\nPhillips 2002; Kavan andˇZ´ara 2005; Merry et al. 2006; Kavan et al.\\n2008]. There has also been a lot of work on learning highly realis-\\ntic body models from data [Allen et al. 2006; Anguelov et al. 2005;\\nFreifeld and Black 2012; Hasler et al. 2010; Chang and Zwicker\\n2009; Chen et al. 2013]. These methods can capture the body shape\\nof many people as well as non-rigid deformations due to pose. The\\nmost realistic approaches are arguably based on triangle deforma-\\ntions [Anguelov et al. 2005; Chen et al. 2013; Hasler et al. 2010;\\nPons-Moll et al. 2015]. Despite the above research, existing mod-'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='Figure 2: Models compared with ground truth.This ﬁgure deﬁnes the color coding used throughout the paper andSupplemental Video.\\nThe far right (light gray) mesh is a 3D scan. Next to it (dark gray) is a registered mesh with the same topology as our model. We ask how\\nwell different models can approximate this registration. From left to right: (light green) Linear blend skinning (LBS), (dark green) Dual-\\nquaternion blend skinning (DQBS), (blue) BlendSCAPE, (red) SMPL-LBS, (orange) SMPL-DQBS. The zoomed regions highlight differences\\nbetween the models at the subject’s right elbow and hip. LBS and DQBS produce serious artifacts at the knees, elbows, shoulders and hips.\\nBlendSCAPE and both SMPL models do similarly well at ﬁtting the data.\\nels either lack realism, do not work with existing packages, do not\\nrepresent a wide variety of body shapes, are not compatible with\\nstandard graphics pipelines, or require signiﬁcant manual labor.\\nIn contrast to the previous approaches, a key goal of our work is to\\nmake the body model as simple and standard as possible so that it\\ncan be widely used, while, at the same time, keeping the realism\\nof deformation-based models learned from data. Speciﬁcally we\\nlearn blend shapes to correct for the limitations of standard skin-\\nning. Different blend shapes for identity, pose, and soft-tissue dy-\\nnamics are additively combined with a rest template before being\\ntransformed by blend skinning. A key component of our approach\\nis that we formulate the pose blend shapes as a linear function of\\nthe elements of the part rotation matrices. This formulation is dif-\\nferent from previous methods [Allen et al. 2006; Merry et al. 2006;\\nWang and Phillips 2002] and makes training and animating with\\nthe blend shapes simple. Because the elements of rotation matrices\\nare bounded, so are the resulting deformations, helping our model\\ngeneralize better.\\nOur formulation admits an objective function that penalizes the per-\\nvertex disparities between registered meshes and our model, en-\\nabling training from data. To learn how people deform with pose,\\nwe use 1786 high-resolution 3D scans of different subjects in a wide\\nvariety of poses. We align our template mesh to each scan to cre-\\nate a training set. We optimize the blend weights, pose-dependent\\nblend shapes, the mean template shape (rest pose), and a regressor\\nfrom shape to joint locations to minimize the vertex error of the\\nmodel on the training set. This joint regressor predicts the location\\nof the joints as a function of the body shape and is critical to ani-\\nmating realistic pose-dependent deformations for any body shape.\\nAll parameters are automatically estimated from the aligned scans.\\nWe learn linear models of male and female body shape from the\\nCAESAR dataset [Robinette et al. 2002] (approximately 2000 scans\\nper gender) using principal component analysis (PCA). We ﬁrst reg-\\nister a template mesh to each scan and pose normalize the data,\\nwhich is critical when learning a vertex-based shape model. The\\nresulting principal components become body shape blend shapes.\\nWe train the SMPL model in various forms and compare it quantita-\\ntively to a BlendSCAPE model [Hirshberg et al. 2012] trained with\\nexactly the same data. We evaluate the models both qualitatively\\nwith animations and quantitatively using meshes that were not used\\nfor training. We ﬁt SMPL and BlendSCAPE to these meshes and\\nthen compare the vertex errors. Two main variants of SMPL are\\nexplored, one using linear blend skinning (LBS) and the other with\\nDual-Quaternion blend skinning (DQBS); see Fig. 2. The surprise\\nis that a vertex-based, skinned, model such as SMPL is actually\\nmore accurate than a deformation-based model like BlendSCAPE\\ntrained on the same data. The test meshes are available for research\\npurposes so others can quantitatively compare to SMPL.\\nWe extend the SMPL model to capture soft-tissue dynamics by\\nadapting the Dyna model [Pons-Moll et al. 2015]. The resulting\\nDynamic-SMPL, or DMPL model, is trained from the same dataset\\nof 4D meshes as Dyna. DMPL, however, is based on vertices in-\\nstead of triangle deformations. We compute vertex errors between\\nSMPL and Dyna training meshes, transformed into the rest pose,\\nand use PCA to reduce the dimensionality, producing dynamic\\nblend shapes. We then train a soft-tissue model based on angu-\\nlar velocities and accelerations of the parts and the history of dy-\\nnamic deformations as in [Pons-Moll et al. 2015]. Since soft-tissue\\ndynamics strongly depend on body shape, we train DMPL using\\nbodies of varying body mass index and learn a model of dynamic\\ndeformations that depends of body shape. Animating soft-tissue\\ndynamics in a standard rendering engine simply requires comput-\\ning the dynamic linear blend shape coefﬁcients from the sequence\\nof poses. Side-by-side animations of Dyna and DMPL reveal that\\nDMPL is more realistic. This extension of SMPL illustrates the\\ngenerality of our additive blend shape approach, shows how defor-\\nmations can depend on body shape, and demonstrates how the ap-\\nproach provides a extensible foundation for modeling body shape.\\nSMPL models can be animated signiﬁcantly faster than real time\\non a CPU using standard rendering engines. Consequently SMPL\\naddresses an open problem in the ﬁeld; it makes a realistic learned\\nmodel accessible to animators. The SMPL base template is de-'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='signed with animation in mind; it has a low-polygon count, a simple\\nvertex topology, clean quad structure, a standard rig, and reason-\\nable face and hand detail (though we do not rig the hands or face\\nhere). SMPL can be represented as an Autodesk Filmbox (FBX) ﬁle\\nthat can be imported into animation systems. We make the SMPL\\nmodel available for research purposes and provide scripts to drive\\nour model in Maya, Blender, Unreal Engine and Unity.\\n2 Related Work\\nLinear blend skinning and blend shapes are widely used throughout\\nthe animation industry. While the research community has pro-\\nposed many novel models of articulated body shape that produce\\nhigh-quality results, they are not compatible with industry practice.\\nMany authors have tried to bring these worlds together with varying\\ndegrees of success as we summarize below.\\nBlend Skinning. Skeleton subspace deformation methods, also\\nknown as blend skinning, attach the surface of a mesh to an un-\\nderlying skeletal structure. Each vertex in the mesh surface is\\ntransformed using a weighted inﬂuence of its neighboring bones.\\nThis inﬂuence can be deﬁned linearly as in Linear Blend Skinning\\n(LBS). The problems of LBS have been widely published and the\\nliterature is dense with generic methods that attempt to ﬁx these,\\nsuch as quaternion or dual-quaternion skinning, spherical skinning,\\netc. (e.g. [Wang and Phillips 2002; Kavan andˇZ´ara 2005; Kavan\\net al. 2008; Le and Deng 2012; Wang et al. 2007]). Generic meth-\\nods, however, often produce unnatural results and here we focus on\\nlearning to correct the limitations of blend skinning, regardless of\\nthe particular formulation.\\nAuto-rigging. There is a great deal of work on automatically rig-\\nging LBS models (e.g. [De Aguiar et al. 2008; Baran and Popovi´c\\n2007; Corazza and Gambaretto 2014; Schaefer and Yuksel 2007])\\nand commercial solutions exist. Most relevant here are methods that\\ntake a collection of meshes and infer the bones as well as the joints\\nand blend weights (e.g. [Le and Deng 2014]). Such methods do not\\naddress the common problems of LBS models because they do not\\nlearn corrective blend shapes. Models created from sequences of\\nmeshes (e.g. [De Aguiar et al. 2008]) may not generalize well to\\nnew poses and motions. Here, we assume the kinematic structure\\nis known, though the approach could be extended to also learn this\\nusing the methods above.\\nThe key limitation of the above methods is that the models do not\\nspan a space of body shapes. Miller et al. [2010] partially address\\nthis by auto-rigging using a database of pre-rigged models. They\\nformulate rigging and skinning as the process of transferring and\\nadapting skinning weights from known models to a new model.\\nTheir method does not generate blend shapes, produces standard\\nLBS artifacts, and does not minimize a clear objective function.\\nBlend shapes. To address the shortcomings of basic blend skin-\\nning, the pose space deformation model (PSD) [Lewis et al. 2000]\\ndeﬁnes deformations (as vertex displacements) relative to a base\\nshape, where these deformations are a function of articulated pose.\\nThis is the key formulation that is largely followed by later ap-\\nproaches and is also referred to as “scattered data interpolation” and\\n“corrective enveloping” [Rouet and Lewis 1999]. We take an ap-\\nproach more similar to weighted pose-space deformation (WPSD)\\n[Kurihara and Miyata 2004; Rhee et al. 2006], which deﬁnes the\\ncorrections in a rest pose and then applies a standard skinning equa-\\ntion (e.g. LBS). The idea is to deﬁne corrective shapes (sculpts) for\\nspeciﬁc key poses, so that when added to the base shape and trans-\\nformed by blend skinning, produce the right shape. Typically one\\nﬁnds the distance (in pose space) to the exemplar poses and uses a\\nfunction, e.g. a Radial Basis (RBF) kernel [Lewis et al. 2000], to\\nweight the exemplars non-linearly based on distance. The sculpted\\nblend shapes are then weighted and linearly combined.\\nThese approaches are all based on computing weighted distances\\nto exemplar shapes. Consequently, these methods require compu-\\ntation of the distances and weights at runtime to obtain the cor-\\nrective blend shape. For a given animation (e.g. in a video game)\\nthese weights are often deﬁned in advance based on the poses and\\n“baked” into the model. Game engines apply the baked-in weights\\nto the blend shapes. The sculpting process is typically done by an\\nartist and then only for poses that will be used in the animation.\\nLearning pose models.Allen et al. [2002] use this PSD approach\\nbut rather than hand-sculpt the corrections, learn them from regis-\\ntered 3D scans. Their work focuses primarily on modeling the torso\\nand arms of individuals, rather than whole bodies of a population.\\nThey store deformations of key poses and interpolate between them.\\nWhen at, or close to, a stored shape, these methods are effectively\\nperfect. They do not tend to generalize well to new poses, requiring\\ndense training data. It is not clear how many such shapes would\\nbe necessary to model the full range of articulated human pose. As\\nthe complexity of the model increases, so does the complexity of\\ncontrolling all these shapes and how they interact.\\nTo address this, Kry et al. [2002] learn a low-dimensional PCA\\nbasis for each joint’s deformations. Pose-dependent deformations\\nare described in terms of the coefﬁcients of the basis vectors. Ka-\\nvan et al. [2009] use example meshes generated using a non-linear\\nskinning method to construct linear approximations. James and\\nTwigg [2005] combine the idea of learning the bones (non-rigid,\\nafﬁne bones) and skinning weights directly from registered meshes.\\nFor blend shapes they use an approach similar to [Kry et al. 2002].\\nAnother way to address the limitations of blend skinning is through\\n“multi-weight enveloping” (MWE) [Wang and Phillips 2002].\\nRather than weight each vertex by a weighted combination of the\\nbone transformation matrices, MWE learns weights for theele-\\nments of these matrices. This increases the capacity of the model\\n(more parameters). Like [James and Twigg 2005] they overparam-\\neterize the bone transformations to give more expressive power and\\nthen use PCA to remove unneeded degrees of freedom. Their ex-\\nperiments typically involve user interaction and the MWE approach\\nis not supported by current game engines.\\nMerry et al. [2006] ﬁnd MWE to be overparameterized, because it\\nallows vertices to deform differently depending on rotation in the\\nglobal coordinate system. Their Animation Space model reduces\\nthe parameterization at minimal loss of representational power,\\nwhile also showing computational efﬁciency on par with LBS.\\nAnother alternative is proposed by Mohr and Gleicher [2003] who\\nlearn an efﬁcient linear and realistic model from example meshes.\\nTo deal with the problems of LBS, however, they introduce ex-\\ntra “bones” to capture effects like muscle bulging. These extra\\nbones increase complexity, are non-physical, and are non-intuitive\\nfor artists. Our blend shapes are simpler, more intuitive, more prac-\\ntical, and offer greater realism. Similarly, Wang et al. [2007] in-\\ntroduce joints related to surface deformation. Their rotational re-\\ngression approach uses deformation gradients, which then must be\\nconverted to a vertex representation.\\nLearning pose and shape models.The above methods focus on\\nlearning poseable single-shape models. Our goal, however, is to\\nhave realistic poseable models that cover the space of human shape\\nvariation. Early methods use PCA to characterize a space of human\\nbody shapes [Allen et al. 2003; Seo et al. 2003] but do not model\\nhow body shape changes with pose. The most successful class of\\nmodels are based on SCAPE [Anguelov et al. 2005] and represent\\nbody shape and pose-dependent shape in terms of triangle deforma-'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='tions rather than vertex displacements [Chen et al. 2013; Freifeld\\nand Black 2012; Hasler et al. 2009; Hirshberg et al. 2012; Pons-\\nMoll et al. 2015]. These methods learn statistical models of shape\\nvariation from training scans containing different body shapes and\\nposes. Triangle deformations provide allow the composition of dif-\\nferent transformations such as body shape variation, rigid part rota-\\ntion, and pose-dependent deformation. Weber et al. [2007] present\\nan approach that has properties of SCAPE but blends this with ex-\\nample shapes. These models are not consistent with existing ani-\\nmation software.\\nHasler et al. [2010] learn two linear blend rigs: one for pose and one\\nfor body shape. To represent shape change, they introduce abstract\\n“bones” that control the shape change of the vertices. Animating\\na character of a particular shape involves manipulating the shape\\nand pose bones. They learn a base mesh and blend weights but not\\nblend shapes. Consequently the model lacks realism.\\nWhat we would like is a vertex-based model that has the expres-\\nsive power of the triangle deformation models so that it can cap-\\nture a whole range of natural shapes and poses. Allen et al. [2006]\\nformulate such a model. For a given base body shape, they de-\\nﬁne a standard LBS model with scattered/exemplar PSD to model\\npose deformations (using RBFs). They greedily deﬁne “key an-\\ngles” at which to represent corrective blend shapes and they hold\\nthese ﬁxed across all body shapes. A given body shape is param-\\neterized by the vertices of the rest pose, corrective blend shapes\\n(at the key angles), and bone lengths; these comprise a “character\\nvector.” Given different character vectors for different bodies they\\nlearn a low-dimensional latent space that lets them generalize char-\\nacter vectors to new body shapes; they learn these parameters from\\ndata. Their model is more complex than ours, has fewer parame-\\nters, and is learned from much less data. A more detailed analysis\\nof how this method compares to SMPL is presented in Sec. 7.\\n3 Model Formulation\\nOur Skinned Multi-Person Linear model (SMPL) is illustrated in\\nFig. 3. Like SCAPE, the SMPL model decomposes body shape\\ninto identity-dependent shape and non-rigid pose-dependent shape;\\nunlike SCAPE, we take a vertex-based skinning approach that uses\\ncorrective blend shapes. A single blend shape is represented as\\na vector of concatenated vertex offsets. We begin with an artist-\\ncreated mesh withN =6 8 9 0vertices and K =2 3 joints. The\\nmesh has the same topology for men and women, spatially varying\\nresolution, a clean quad structure, a segmentation into parts, initial\\nblend weights, and a skeletal rig. The part segmentation and initial\\nblend weights are shown in Fig. 6.\\nFollowing standard skinning practice, the model is deﬁned by a\\nmean template shape represented by a vector ofN concatenated\\nvertices ¯T 2 R3N in the zero pose,~✓⇤; a set of blend weights,W 2\\nRN⇥K, (Fig. 3(a)); a blend shape function,BS(~\\x00): R|~\\x00| 7! R3N ,\\nthat takes as input a vector of shape parameters,~\\x00, (Fig. 3(b)) and\\noutputs a blend shape sculpting the subject identity; a function to\\npredict K joint locations (white dots in Fig. 3(b)),J(~\\x00): R|~\\x00| 7!\\nR3K as a function of shape parameters,~\\x00; and a pose-dependent\\nblend shape function,BP (~✓): R|~✓| 7! R3N , that takes as input a\\nvector of pose parameters,~✓, and accounts for the effects of pose-\\ndependent deformations (Fig. 3(c)). The corrective blend shapes of\\nthese functions are added together in the rest pose as illustrated in\\n(Fig. 3(c)). Finally, a standard blend skinning functionW(· ) (linear\\nor dual-quaternion) is applied to rotate the vertices around the es-\\ntimated joint centers with smoothing deﬁned by the blend weights.\\nThe result is a model,M(~\\x00, ~✓; \\x00): R|~✓|⇥|~\\x00| 7! R3N , that maps\\nshape and pose parameters to vertices (Fig. 3(d)). Here\\x00 represents\\nthe learned model parameters described below.\\nBelow we will use both LBS and DQBS skinning methods. In gen-\\neral the skinning method can be thought of as a generic black box.\\nGiven a particular skinning method our goal is to learn\\x00 to correct\\nfor limitations of the method so as to model training meshes. Note\\nthat the learned pose blend shapes both correct errors caused by the\\nblend skinning function and static soft-tissue deformations caused\\nby changes in pose.\\nBelow we describe each term in the model. For convenience, a\\nnotational summary is provided in Table 1 in the Appendix.\\nBlend skinning. To ﬁx ideas and deﬁne notation, we present the\\nLBS version as it makes exposition clear (the DQBS version of\\nSMPL only requires changing the skinning equation). Meshes and\\nblend shapes are vectors of vertices represented by bold capital let-\\nters (e.g.X) and lowercase bold letters (e.g.xi 2 R3) are vectors\\nrepresenting a particular vertex. The vertices are sometimes repre-\\nsented in homogeneous coordinates. We use the same notation for\\na vertex whether it is in standard or homogeneous coordinates as it\\nshould always be clear from the context which form is needed.\\nThe pose of the body is deﬁned by a standard skeletal rig, where\\n~!k 2 R3 denotes the axis-angle representation of the relative rota-\\ntion of partk with respect to its parent in the kinematic tree. Our\\nrig hasK =2 3joints, hence a pose~✓ =[ ~!T\\n0 ,..., ~!T\\nK]T is deﬁned\\nby |~✓| =3 ⇥ 23 + 3 = 72parameters; i.e.3 for each part plus3\\nfor the root orientation. Let¯! = ~!\\nk~!k denote the unit norm axis of\\nrotation. Then the axis angle for every jointj is transformed to a\\nrotation matrix using theRodrigues formula\\nexp(~!j)= I + b¯!j sin(k~!jk)+ b¯!\\n2\\nj cos(k~!jk) (1)\\nwhere b¯! is the skew symmetric matrix of the 3-vector¯! and I is\\nthe 3 ⇥3 identity matrix. Using this, the standard linear blend skin-\\nning function W(¯T, J, ~✓, W): R3N⇥3K⇥|~✓|⇥|W| 7! R3N takes\\nvertices in the rest pose,¯T, joint locations,J, a pose,~✓, and the\\nblend weights, W, and returns the posed vertices. Each vertex¯ti\\nin ¯T is transformed into¯t0\\ni (both column vectors in homogeneous\\ncoordinates) as\\n¯t0\\ni =\\nKX\\nk=1\\nwk,iG0\\nk(~✓, J)¯ti (2)\\nG0\\nk(~✓, J)= Gk(~✓, J)Gk(~✓⇤, J)\\x001 (3)\\nGk(~✓, J)=\\nY\\nj2A(k)\\n\\uf8ff exp(~!j) jj\\n~0 1\\n\\x00\\n(4)\\nwhere wk,i is an element of the blend weight matrixW, represent-\\ning how much the rotation of partk effects the vertexi, exp(~✓j) is\\nthe local3 ⇥3 rotation matrix corresponding to jointj, Gk(~✓, J) is\\nthe world transformation of jointk, andG0\\nk(~✓, J) is the same trans-\\nformation after removing the transformation due to the rest pose,\\n~✓⇤. Each 3-element vector inJ corresponding to a single joint cen-\\nter, j, is denotedjj. Finally, A(k) denotes the ordered set of joint\\nancestors of jointk. Note, for compatibility with existing render-\\ning engines, we assumeW is sparse and allow at most four parts to\\ninﬂuence a vertex.\\nMany methods have modiﬁed equation (2) to make skinning more\\nexpressive. For example MWE [Wang and Phillips 2002] replaces\\nGk(~✓, J) with a more general afﬁne transformation matrix and re-\\nplaces the scalar weight with a separate weight for every element of'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='(a) ¯T, W\\n (b) ¯T + BS(~\\x00), J(~\\x00)\\n (c) TP (~\\x00, ~✓)= ¯T+BS(~\\x00)+BP (~✓)\\n (d) W(TP (~\\x00, ~✓),J (~\\x00), ~✓, W)\\nFigure 3: SMPL model. (a) Template mesh with blend weights indicated by color and joints shown in white. (b) With identity-driven\\nblendshape contribution only; vertex and joint locations are linear in shape vector~\\x00. (c) With the addition of of pose blend shapes in\\npreparation for the split pose; note the expansion of the hips. (d) Deformed vertices reposed by dual quaternion skinning for the split pose.\\nthe transformation matrix. Such changes are expressive but are not\\ncompatible with existing animation systems.\\nTo maintain compatibility, we keep the basic skinning function and\\ninstead modify the template in an additive way and learn a function\\nto predict joint locations. Our model,M(~\\x00, ~✓; \\x00) is then\\nM(~\\x00, ~✓)= W(TP (~\\x00, ~✓),J (~\\x00), ~✓, W) (5)\\nTP (~\\x00, ~✓)= ¯T + BS(~\\x00)+ BP (~✓) (6)\\nwhere BS(~\\x00) and BP (~✓) are vectors of vertices representing off-\\nsets from the template. We refer to these as shape and pose blend\\nshapes respectively.\\nGiven this deﬁnition, a vertex¯ti is transformed according to\\n¯t0\\ni =\\nKX\\nk=1\\nwk,iG0\\nk(~✓,J (~\\x00))(¯ti + bS,i(~\\x00)+ bP,i(~✓)) (7)\\nwhere bS,i(~\\x00), bP,i(~✓) are vertices inBS(~\\x00) and BP (~✓) respec-\\ntively and represent the shape and pose blend shape offsets for the\\nvertex¯ti. Hence, the joint centers are now a function of body shape\\nand the template mesh that is deformed by blend skinning is now a\\nfunction of both pose and shape. Below we describe each term in\\ndetail.\\nShape blend shapes.The body shapes of different people are rep-\\nresented by a linear functionBS\\nBS(~\\x00; S)=\\n|~\\x00|X\\nn=1\\n\\x00nSn (8)\\nwhere ~\\x00 =[ \\x001,..., \\x00|~\\x00|]T , |~\\x00| is the number of linear shape coefﬁ-\\ncients, and theSn 2 R3N represent orthonormal principal compo-\\nnents of shape displacements. LetS =[ S1,..., S|~\\x00|] 2 R3N⇥|~\\x00|\\nbe the matrix of all such shape displacements. Then the linear func-\\ntion BS(~\\x00; S) is fully deﬁned by the matrixS, which is learned\\nfrom registered training meshes, see Sec. 4.\\nNotationally, the values to the right of a semicolon represent learned\\nparameters, while those on the left are parameters set by an anima-\\ntor. For notational convenience, we often omit the learned parame-\\nters when they are not explicitly being optimized in training.\\nFigure 3(b) illustrates the application of these shape blend shapes\\nto the template¯T to produce a new body shape.\\nPose blend shapes. We denote asR : R|~✓| 7! R9K a function\\nthat maps a pose vector~✓ to a vector of concatenated part relative\\nrotation matrices,exp(~!). Given that our rig has23 joints, R(~✓) is\\na vector of length(23 ⇥ 9=2 0 7 ). Elements ofR(~✓) are functions\\nof sines and cosines (Eq. (1)) of joint angles and thereforeR(~✓) is\\nnon-linear with~✓.\\nOur formulation differs from previous work in that we deﬁne the\\neffect of the pose blend shapes to be linear inR⇤(~✓)=( R(~✓) \\x00\\nR(~✓⇤)), where~✓⇤ denotes the rest pose. LetRn(~✓) denote thenth\\nelement ofR(~✓), then the vertex deviations from the rest template\\nare\\nBP (~✓; P)=\\n9KX\\nn=1\\n(Rn(~✓) \\x00 Rn(~✓⇤))Pn, (9)\\nwhere the blend shapes,Pn 2 R3N , are again vectors of vertex\\ndisplacements. Here P =[ P1,..., P9K] 2 R3N⇥9K is a matrix\\nof all 207 pose blend shapes. In this way, the pose blend shape\\nfunction BP (~✓; P) is fully deﬁned by the matrixP, which we learn\\nin Sec. 4.\\nNote that subtracting the rest pose rotation vector,R(~✓⇤), guaran-\\ntees that the contribution of the pose blend shapes is zero in the rest\\npose, which is important for animation.\\nJoint locations. Different body shapes have different joint loca-\\ntions. Each joint is represented by its 3D location in the rest pose.\\nIt is critical that these are accurate, otherwise there will be artifacts\\nwhen the model is posed using the skinning equation. For that rea-\\nson, we deﬁne the joints as a function of the body shape,~\\x00,\\nJ(~\\x00; J , ¯T, S)= J (¯T + BS(~\\x00; S)) (10)\\nwhere J is a matrix that transforms rest vertices into rest joints.\\nWe learn the regression matrix,J , from examples of different peo-\\nple in many poses, as part of our overall model learning in Sec. 4.\\nThis matrix models which mesh vertices are important and how to\\ncombine them to estimate the joint locations.\\nSMPL model.We can now specify the full set of model parameters\\nof the SMPL model as\\x00 =\\n\\x00¯T, W, S, J , P\\n \\n. We describe how\\nto learn these in Sec. 4. Once learned they are held ﬁxed and new'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='Figure 4:Sample registrations from the multipose dataset.\\nbody shapes and poses are created and animated by varying~\\x00 and\\n~✓ respectively.\\nThen the SMPL model is ﬁnally deﬁned asM(~\\x00, ~✓; \\x00)=\\nW\\n⇣\\nTP (~\\x00, ~✓; ¯T, S, P),J (~\\x00; J , ¯T, S), ~✓, W\\n⌘\\n(11)\\nand hence each vertex is transformed as\\nt0\\ni =\\nKX\\nk=1\\nwk,iG0\\nk(~✓,J (~\\x00; J , ¯T, S))tP,i(~\\x00, ~✓; ¯T, S, P) (12)\\nwhere tP,i(~\\x00, ~✓; ¯T, S, P)=\\n¯ti +\\n|~\\x00|X\\nm=1\\n\\x00msm,i +\\n9KX\\nn=1\\n(Rn(~✓) \\x00 Rn(~✓⇤))pn,i (13)\\nrepresents the vertexi after applying the blend shapes and where\\nsm,i, pn,i 2 R3 are the elements of the shape and pose blend\\nshapes corresponding to template vertex¯ti.\\nBelow we experiment with both LBS and DQBS and train the pa-\\nrameters for each. We refer to these models as SMPL-LBS and\\nSMPL-DQBS; SMPL-DQBS is our default model, and we use\\nSMPL as shorthand to mean SMPL-DQBS.\\n4 Training\\nWe train the SMPL model parameters to minimize reconstruction\\nerror on two datasets. Each dataset contains meshes with the same\\ntopology as our template that have been aligned to high-resolution\\n3D scans using [Bogo et al. 2014]; we call these aligned meshes\\n“registrations.” The multi-pose dataset consists of1786 registra-\\ntions of40 individuals (891 registrations spanning20 females, and\\n895 registrations spanning20 males); a sampling is shown in Fig. 4.\\nThe multi-shape dataset consists of registrations to the CAESAR\\ndataset [Robinette et al. 2002], totaling 1700 registrations for males\\nand 2100 for females; a few examples are shown in Fig. 5. We de-\\nnote thejth mesh in the multi-pose dataset asVP\\nj and thejth mesh\\nin the multi-shape dataset asVS\\nj .\\nOur goal is to train the parameters\\x00 =\\n\\x00¯T, W, S, J , P\\n \\nto mini-\\nmize vertex reconstruction error on the datasets. Because our model\\ndecomposes shape and pose, we train these separately, simplify-\\ning optimization. We ﬁrst train{J , W, P} using our multi-pose\\ndataset and then train{¯T, S} using our multi-shape dataset. We\\ntrain separate models for men and women (i.e.\\x00m and \\x00f ).\\nFigure 5:Sample registrations from the multishape dataset.\\n4.1 Pose Parameter Training\\nWe ﬁrst use the multi-pose dataset to train{J , W, P}. To this\\nend, we need to compute the rest templates,ˆTP\\ni , and joint loca-\\ntions, ˆJP\\ni , for each subject,i, as well as the pose parameters,~✓j, for\\neach registration,j, in the dataset. We alternate between optimiz-\\ning registration speciﬁc parameters~✓j, subject-speciﬁc parameters\\n{ˆTP\\ni , ˆJP\\ni }, and global parameters{W, P}. We then learn the ma-\\ntrix, J , to regress from subject-speciﬁc vertex locations,ˆTP\\ni , to\\nsubject-speciﬁc joint locations,ˆJP\\ni . To achieve all this, we mini-\\nmize an objective function consisting of a data term,ED, and sev-\\neral regularization terms{EJ,E Y ,E P ,E W } deﬁned below.\\nThe data term penalizes the squared Euclidean distance between\\nregistration vertices and model vertices\\nED(ˆTP , ˆJP , W, P, ⇥)=\\nPregX\\nj=1\\n||VP\\nj \\x00 W(ˆTP\\ns(j) + BP (~✓j; P), ˆJP\\ns(j), ~✓j, W)||2\\nwhere ⇥ =\\nn\\n~✓1,..., ~✓Preg\\no\\n, s(j) is the subject index correspond-\\ning to registrationj, Preg are the number of meshes in the pose\\ntrainings set, ˆTP = {ˆTP\\ni }\\nPsubj\\ni=1 , ˆJP = {ˆJP\\ni }\\nPsubj\\ni=1 are the sets of\\nall rest poses and joints, andPsubj is the number of subjects in the\\npose training set.\\nWe estimate207 ⇥3 ⇥6890 = 4, 278, 690 parameters for the pose\\nblend shapes,P, 4 ⇥ 3 ⇥ 6890 = 82, 680 parameters for the skin-\\nning weights,W, and3⇥6890⇥23⇥3=1 , 426, 230 for the joint\\nregressor matrix,J . To make the estimation well behaved, we reg-\\nularize by making several assumptions. A symmetry regularization\\nterm, EY , penalizes left-right asymmetry forˆJP and ˆTP\\nEY (ˆJP , ˆTP )=\\nPsubjX\\ni=1\\n\\x00U ||ˆJP\\ni \\x00 U(ˆJP\\ni )||2 + ||ˆTP\\ni \\x00 U(ˆTP\\ni )||2,\\nwhere \\x00U =1 0 0, and whereU(T) ﬁnds a mirror image of vertices\\nT, by ﬂipping across the sagittal plane and swapping symmetric\\nvertices. This term encourages symmetric template meshes and,\\nmore importantly, symmetric joint locations. Joints are unobserved\\nvariables and along the spine they are particularly difﬁcult to lo-\\ncalize. While models trained without the symmetry term produce'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='(a) Segmentation\\n (b) Initialization WI\\nFigure 6: Initialization of joints and blend weights.Discrete\\npart segmentation in (a) is diffused to obtain initial blend weights,\\nWI, in (b). Initial joint centers are shown as white dots.\\nreasonable results, enforcing symmetry produces models that are\\nvisually more intuitive for animation.\\nOur model is hand segmented into 24 parts (Fig. 6). We use this\\nsegmentation to compute an initial estimate of the joint centers and\\na regressorJI from vertices to these centers. This regressor com-\\nputes the initial joints by taking the average of the ring of vertices\\nconnecting two parts. When estimating the joints for each subject\\nwe regularize them to be close to this initial prediction:\\nEJ(ˆTP , ˆJP )=\\nPsubjX\\ni=1\\n||JI ˆTP\\ni \\x00 ˆJP\\ni ||2.\\nTo help prevent overﬁtting of the pose-dependent blend shapes, we\\nregularize them towards zero\\nEP (P)= ||P||2\\nF ,\\nwhere k · kF denotes the Frobenius norm. Note that replacing the\\nquadratic penalty with an L1 penalty would encourage greater spar-\\nsity of the blend shapes. We did not try this.\\nWe also regularize the blend weights towards the initial weights,\\nWI, shown in Fig. 6:\\nEW (W)= ||W \\x00 WI||2\\nF .\\nThe initial weights are computed by simply diffusing the segmen-\\ntation.\\nAltogether, the energy for training{W, P} is as follows:\\nE⇤(ˆTP , ˆJP , ⇥, W, P)=\\nED + \\x00Y EY + \\x00JEJ + \\x00P EP + EW , (14)\\nwhere \\x00Y =1 0 0, \\x00J =1 0 0and \\x00P =2 5. These weights were set\\nempirically. Our model has a large number of parameters and the\\nFigure 7: Joint regression.(left) Initialization. Joint locations can\\nbe inﬂuenced by locations on the surface, indicated by the colored\\nlines. We assume that these inﬂuences are somewhat local. (right)\\nOptimized. After optimization we ﬁnd a sparse set of vertices and\\nassociated weights inﬂuencing each joint.\\nregularization helps prevent overﬁtting. As the size of the training\\nset grows, so does the strength of the data term, effectively reduc-\\ning the inﬂuence of the regularization terms. Our experiments be-\\nlow with held-out test data suggest that the learned models are not\\noverﬁt to the data and generalize well.\\nThe overall optimization strategy is described in Sec. 4.3.\\nJoint regressor. Optimizing the above gives a template mesh\\nand joint locations for each subject, but we want to predict joint\\nlocations for new subjects with new body shapes. To that end, we\\nlearn the regressor matrixJ to predict the training joints from the\\ntraining bodies. We tried several regression strategies; what we\\nfound to work best, was to computeJ using non-negative least\\nsquares [Lawson and Hanson 1995] with the inclusion of a term that\\nencourages the weights to add to one. This approach encourages\\nsparsity of the vertices used to predict the joints. Making weights\\npositive and add to one discourages predicting joints outside the\\nsurface. These constraints enforce the predictions to be in the con-\\nvex hull of surface points. Figure 7 shows the non-zero elements\\nof the regression matrix, illustrating that a sparse set of surface ver-\\ntices are linearly combined to estimate the joint centers.\\n4.2 Shape Parameter Training\\nOur shape space is deﬁned by a mean and principal shape directions\\n{¯T, S}. It is computed by running PCA on shape registrations from\\nour multi-shape database after pose normalization. Pose normaliza-\\ntion transforms a raw registrationVS\\nj into a registration,ˆTS\\nj , in the\\nrest pose,~✓⇤. This normalization is critical to ensure that pose and\\nshape are modeled separately.\\nTo pose-normalize a registration,VS\\nj , we ﬁrst have to estimate\\nits pose. We denote ˆTP\\nµ and ˆJP\\nµ as the mean shape and mean\\njoint locations from the multi-pose database respectively. Let\\nWe(ˆTP\\nµ , ˆJP\\nµ , ~✓, W), VS\\nj,e 2 R3 denote an edge of the model and\\nof the registration respectively. An edge is obtained by subtracting\\na pair of neighboring vertices. To estimate the pose using an aver-\\nage generic shapeˆTP\\nµ , we minimize the following sum of squared\\nedge differences so that~✓j =\\narg min\\n~✓\\nX\\ne\\n||We(ˆTP\\nµ + BP (~✓; P), ˆJP\\nµ , ~✓, W) \\x00 VS\\nj,e||2, (15)\\nwhere the sum is taken over all edges in the mesh. This allows us to\\nget a good estimate of the pose without knowing the subject speciﬁc\\nshape.'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='(a) PC 1\\n (b) PC 2\\n (c) PC 3\\nFigure 8: Shape blend shapes.The ﬁrst three shape principal components of body shape are shown. PC1 and PC2 vary from -2 to +2\\nstandard deviations from left to right, while PC3 varies from -5 to +5 standard deviations to make the shape variation more visible. Joint\\nlocations (red dots) vary as a function of body shape and are predicted using the learned regressor,J .\\nOnce the pose~✓j is known we solve forˆTS\\nj by minimizing\\nˆTS\\nj = arg min\\nˆT\\n||W( ˆT + BP (~✓j; P), J ˆT, ~✓j, W) \\x00 VS\\nj ||2.\\nThis computes the shape that, when posed, matches the training\\nregistration. This shape is the pose-normalized shape.\\nWe then run PCA on{ˆTS\\nj }\\nSsubj\\nj=1 to obtain{¯T, S}. This procedure\\nis designed to maximize the explained variance of vertex offsets in\\nthe rest pose, given a limited number of shape directions.\\nNote that the optimization of pose is critically important when\\nbuilding a shape basis from vertices. Without this step, pose vari-\\nations of the subjects in the shape training dataset would be cap-\\ntured in the shape blend shapes. The resulting model would not be\\ndecomposed properly into shape and pose. Note also that this ap-\\nproach constrasts with SCAPE or BlendSCAPE, where PCA is per-\\nformed in the space of per-triangle deformations. Unlike vertices,\\ntriangle deformations do not live in a Euclidean space [Freifeld and\\nBlack 2012]. Hence PCA on vertices is more principled and is con-\\nsistent with the registration data term, which consists of squared\\nvertex disparities.\\nFigure 8 visualizes the ﬁrst three shape components. The ﬁgure\\nalso shows how the joint locations change with the changes in body\\nshape. The joint positions are shown by the spheres and are com-\\nputed from the surface meshes using the learned joint regression\\nfunction. The lines connecting the joints across the standard devia-\\ntions illustrate how the joint positions vary linearly with shape.\\nFigure 9 shows the relative cumulative variance of SMPL and\\nBlendSCAPE. While SMPL requires many fewer PCs to account\\nfor the same percentage of overall variance, the variance is different\\nin the two cases: one is variance in vertex locations and the other\\nis variance in triangle deformations. Explained variance in defor-\\nmations does not directly translate into explained variance in vertex\\nlocations. While this makes the two models difﬁcult to compare\\nprecisely, triangle deformations have many more degrees of free-\\ndom and it is likely that there are many deformations that produce\\nvisually similar shapes. A model requiring fewer components is\\ngenerally preferable.\\n4.3 Optimization summary\\nPose parameters ~✓j in Eq. (14) are ﬁrst initialized by minimiz-\\ning the difference between the model and the registration edges,\\nFigure 9:Cumulative relative variance of the CAESAR dataset ex-\\nplained as a function of the number of shape coefﬁcients. For SMPL\\nthe variance is in vertex locations, while for BlendSCAPE it is in\\ntriangle deformations.\\nsimilar to Eq. (15) using an average template shape. Then\\n{ˆTP , ˆJP , W, P, ⇥} are estimated in an alternating manner to min-\\nimize Eq. 14. We proceed to estimateJ from {ˆJP , ˆTP }.W e\\nthen run PCA on pose normalized subjects{ˆTS\\nj }\\nSsubj\\nj=1 to obtain\\n{¯T, S}. The ﬁnal model is deﬁned by{J , W, P, ¯T, S}. Note that\\nall training parameters except for{¯T, S} are found with gradient-\\nbased dogleg minimization [Nocedal and Wright 2006]. Gradients\\nare computed with automatic differentiation using the the Chumpy\\nframework [Loper and Black 2014].\\n5 SMPL Evaluation\\nAll training subjects gave prior informed written consent for their\\ndata to be used in creating statistical models for distribution. Reg-\\nistered meshes and identiﬁable subjects shown here are of profes-\\nsional models working under contract.'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='¯T + BS(~\\x00) TP (~\\x00, ~✓) M(~\\x00, ~✓) BlendSCAPE Registration\\nFigure 10:Model ﬁtting with intermediate stages. We ﬁt both BlendSCAPE (blue) and SMPL-LBS,M(~\\x00, ~✓), (red) to registered meshes by\\noptimizing pose and shape.¯T + BS(~\\x00) shows the estimated body shape andTP (~\\x00, ~✓) shows the effects of pose-dependent blend shapes.\\nHere we show SMPL-LBS, becauseTP shows more variation due to pose than SMPL-DQBS.\\n5.1 Quantitative Evaluation\\nWe evaluate two types of error.Model generalizationis the ability\\nof the model to ﬁt to meshes of new people and poses; this tests\\nboth shape and pose blend shapes.Pose generalizationis the abil-\\nity to generalize a shape of an individual to new poses of the same\\nperson; this primarily tests how well pose blend shapes correct skin-\\nning artifacts and pose-dependent deformations. Both are measured\\nby mean absolute vertex-to-vertex distance between the model and\\ntest registrations. For this evaluation we use 120 registered meshes\\nof four women and two men from the public Dyna dataset [Dyn\\n2015]. These meshes contain a variety of body shapes and poses.\\nAll meshes are in alignment with our template and none were used\\nto train our models. Figure 10 (gray) shows four examples of these\\nregistered meshes.\\nWe evaluate SMPL-LBS and SMPL-DQBS. We also compare these\\nwith a BlendSCAPE model [Hirshberg et al. 2012] trained from ex-\\nactly the same data as the SMPL models. The kinematic tree struc-\\nture for SMPL and the BlendSCAPE model are the same: therefore\\nwe have the same number of pose parameters. We also compare the\\nmodels using the same number of shape parameters.\\nTo measure model generalization we ﬁrst ﬁt each model to each\\nregistered mesh, optimizing over shape~\\x00 and pose~✓ to ﬁnd the best\\nﬁt in terms of squared vertex distances. Figure 10 shows ﬁts of the\\nSMPL-LBS (red) and BlendSCAPE (blue) models to the registered\\nmeshes. Both do a good job of ﬁtting the data. The ﬁgure also\\nshows how the model works. Illustrated are the estimated body\\nshape, ¯T+BS(~\\x00), and the effect of applying the pose blend shapes,\\nTP (~\\x00, ~✓).\\nFor pose generalization, we take each indvidual, select one of the\\nestimated body shapes from the generalization task, and then op-\\ntimize the pose, ~✓, for each of the other meshes of that subject,\\nkeeping the body shape ﬁxed. The assumption behind pose gener-\\nalization is that, if a model is properly decomposed into pose and\\nshape, then the model should be able to ﬁt the same subject in a\\ndifferent pose, without readjusting shape parameters. Note that the\\npose blend shapes are trained to ﬁt observed registrations. As such,'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='Figure 11: Model generalization indicates how well we can ﬁt an\\nindependent registration. Mean absolute vertex error versus the\\nnumber of shape coefﬁcients used.\\nthey correct for problems of blend skinning and try to capture pose-\\ndependent deformations. Since the pose blend shapes are not de-\\npendent on body shape, they capture something about the average\\ndeformations in the training set.\\nFigures 11 and 12 show the error of the SMPL models and Blend-\\nSCAPE as a function of the number of body shape coefﬁcients used.\\nThe differences between SMPL and BlendSCAPE are small (on\\nthe order of 0.5mm) but SMPL is more accurate on average. Re-\\nmarkably, SMPL-LBS and SMPL-DQBS are essentially identical\\nin model generalization and SMPL-LBS is actually slightly better\\nat pose generalization. This is surprising because the pose blend\\nshapes have much more to correct with LBS. Possibly the simplic-\\nity of LBS helps with generalization to new poses. This analysis\\nis important because it says that users can choose the simpler and\\nfaster LBS model over the DQBS model.\\nThe plots also show how well standard LBS ﬁts the test data.\\nThis corresponds to the SMPL-LBS model with no pose blend\\nshapes. Not surprisingly, LBS produces much higher error than\\neither BlendSCAPE or SMPL. LBS is not as bad in Fig. 11 be-\\ncause here the model can vary body shape parameters, effectively\\nusing changes in identity to try to explain deformations due to\\npose. Figure 12 uses a ﬁxed body shape and consequently illus-\\ntrates how LBS does not model pose-dependent deformations real-\\nistically. Note that here we do not retrain a model speciﬁcally for\\nLBS and expect such a model would be marginally more accurate.\\n5.2 Sparse SMPL\\nThe pose blend shapes in SMPL are not sparse in the sense that\\na rotation of a part can inﬂuence any vertex of the mesh. With\\nsufﬁcient training data sparsity may emerge from data;e.g. the\\nshoulder corrections will not be inﬂuenced by ankle motions. To\\nmake hand animation more intuitive, and to regularize the model\\nto prevent long-range inﬂuences of joints, we can manually enforce\\nsparsity. To this end, we trained a sparse version of SMPL by using\\nthe same sparsity pattern used for blend weights. In particular, we\\nallow a vertex deviation to be inﬂuenced by at most 4 joints. Since\\nevery joint corresponds to a rotation matrix, the pose blend shape\\ncorresponding to any given vertex will be driven by9 ⇥ 4 numbers\\nFigure 12: Pose generalization error indicates how well a ﬁtted\\nshape generalizes to new poses.\\nas opposed to9 ⇥ 23.\\nThis model is referred to as SMPL-LBS-Sparse in Figs. 11 and 12.\\nIt is consistently less accurate than the regular SMPL-LBS model\\nbut may still be useful to animators. This suggests that SMPL-\\nLBS is not overﬁt to the training data and that sparseness reduces\\nthe capacity of the model. The sparse model sometimes produces\\nslight discontinuities at boundaries were vertices are inﬂuenced by\\ndifferent joint angles. Other strategies to enforce sparsity could be\\nadopted, such as using an L1 prior or enforcing smoothness in the\\npose blend shapes. These approaches, however, would complicate\\nthe training process.\\n5.3 Visual Evaluation\\nFigure 13 illustrates the decomposition of shape parameters~\\x00 and\\npose parameters~✓ in SMPL. Pose is held constant from left to right\\nacross each row while varying the shape. Similarly, the shape of\\neach person is held constant while varying the pose from top to bot-\\ntom in each column. The bodies are reposed using poses from the\\nCMU mocap database [CMU 2000]. Note that the pose-dependent\\ndeformations look natural through a wide range of poses, despite\\nvery different body shapes. This illustrates that the joint regression\\nworks well and that the blend shapes are general.\\nPlease see theSupplemental Videofor many more examples and\\nanimations comparing SMPL and BlendSCAPE, illustrating the\\npose blend shapes, and illustrating body shape and pose variation.\\n5.4 Run-time Performance\\nThe run-time cost of SMPL is dominated by skinning and blend-\\nshape multiplication. Many skinning implementations exist, and\\nwe do not claim to have the fastest. Performance of our own CPU-\\nbased implementation, and a comparison against BlendSCAPE, is\\nshown in Fig. 14. The plot shows the time needed to generate the\\nvertices. Note that our BlendSCAPE rendering makes use of mul-\\ntiple cores, while the SMPL rendering does not; this is why the\\nsystem time for BlendSCAPE is higher than the wall-clock time.\\nNote that here we are showing the cost of changing body shape.\\nFor most applications, this is done once and the shape is then held\\nﬁxed. The cost of animating the mesh then comes from only the'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='Figure 13: Animating SMPL.Decomposition of SMPL parameters into pose and shape: Shape parameters,~\\x00, vary across different subjects\\nfrom left to right, while pose parameters,~✓, vary from top to bottom for each subject.\\npose blend shapes; this cost corresponds to 0 shape coefﬁcients.\\nFor meshes with the same number of vertices, SCAPE will always\\nbe slower. In SMPL each blend shape is of size3N, requiring that\\nmany multiplications per shape. SCAPE uses triangle deformations\\nwith 9 elements per triangle and there are roughly twice as many\\ntriangles as vertices. This results in roughly a factor of6 difference\\nbetween SMPL and SCAPE in terms of basic multiplications.\\n5.5 Compatibility with Rendering Engines\\nBecause SMPL is built on standard skinning, it is compatible with\\nexisting 3D animation software. In particular, for a given body\\nshape, we generate the subject-speciﬁc rest-pose template mesh and\\nskeleton (estimated joint locations) and we export SMPL as a rigged\\nmodel with pose blend shapes in Autodesk’s Filmbox (FBX) ﬁle\\nformat, giving cross-platform compatibility. The model loads as a\\ntypical rigged mesh and can be animated as usual in standard 3D\\nanimation software.\\nPose blend weights can be precomputed, baked into the model, and\\nexported as an animated FBX ﬁle. This kind of ﬁle can be loaded\\ninto animation packages and played directly. We have tested the\\nanimated FBX ﬁles in Maya, Unity, and Blender.\\nPose blend weights can also be computed on the ﬂy given the pose,\\n~✓t, at timet. To enable this, we provide scripts that take the joint\\nangles and compute the pose blend weights. We have tested loading\\nand animating SMPL in Maya 2013, 2014 and 2015. The anima-\\ntor can animate the model using any of the conventional animation\\nmethods typically used in Maya. We will provide a Python script\\nthat runs inside Maya to apply blend-shape corrections to an ani-\\nmated SMPL model. The pose blend shape values can be viewed\\nand/or edited manually within Maya if desired. We have also tested\\nSMPL in Unity. In SMPL, the blend weights range from -1 to +1\\nwhile in Unity they range form 0 to 1. Consequently, we scale and\\nrecenter our weights for compatibility. For runtime computation of'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='Figure 14:Performance of SMPL and BlendSCAPE vary with the\\nnumber of body shape coefﬁcients used. Performance shown here\\nis from a 2014 Macbook Pro.\\npose blend shape coefﬁcients, we provide a C# script that the user\\ncan attach to SMPL’s mesh game object.\\nThe SMPL model with shape and pose blend shapes, and\\nthe evaluation meshes, are available for research purposes at\\nhttp://smpl.is.tue.mpg.de.\\n6 DMPL: Dynamic SMPL\\nWhile SMPL models static soft-tissue deformations with pose it\\ndoes not model dynamic deformations that occur due to body move-\\nment and impact forces with the ground. Given 4D registrations that\\ncontain soft-tissue dynamics, we ﬁt them by optimizing only the\\npose of a SMPL model with a personalized template shape. Dis-\\nplacements between SMPL and the observed meshes correspond\\nto dynamic soft-tissue motions. To model these, we introduce a\\nnew set of additive blend shapes that we calldynamic blend shapes.\\nThese additional displacements are correlated with velocities and\\naccelerations of the body and limbs rather than with pose. We\\nfollow the approach of Dyna [Pons-Moll et al. 2015], using the\\nsame training data, and apply the ideas to our additive vertex-based\\nmodel.\\nLet ~\\x00t =[ ˙~✓t, ¨~✓t, vt, at,~\\x00t\\x001,~\\x00t\\x002] denote the dynamic control\\nvector at timet. It is composed of pose velocities and accelerations\\n˙~✓t, ¨~✓t 2 R|~✓|, root joint velocities and accelerationsvt, at 2 R3\\nand a history of two vectors of predicted dynamic coefﬁcients\\n~\\x00t\\x001,~\\x00t\\x002 2 R|~\\x00|, described below.\\nWe extend our linear formulation from Sec. 3 and simply add\\nthe dynamic blend shape function,BD(~\\x00t, ~\\x00), to the other blend\\nshapes in the rest pose before applying the skinning function. The\\nshape in the zero pose becomes\\nTD(~\\x00, ~✓t, ~\\x00t)= ¯T + BS(~\\x00)+ BP (~✓t)+ BD(~\\x00t, ~\\x00), (16)\\nas illustrated in Fig. 15. Here,BD(~\\x00t, ~\\x00) takes as input the dy-\\nnamic control vector at timet, and shape coefﬁcients~\\x00, and pre-\\ndicts vertex offsets in the rest pose.\\nWhereas in [Pons-Moll et al. 2015] dynamic deformations are mod-\\neled using triangle deformations, DMPL models deformations in\\nvertex space. We build male and female models using roughly\\n40, 000 registered male and female meshes from [Dyn 2015]. We\\ncompute the pose in each frame and the displacements between\\nSMPL and the registration. Using PCA, we obtain a mean and the\\ndynamic blend shapes,µD 2 R3N and D 2 R3N⇥|~\\x00| respectively.\\nWe take|~\\x00| =3 0 0principal components as in Dyna. Dynamic de-\\nformations vary signiﬁcantly between subjects based on their body\\nshape and fat distribution. To capture this, we train a model that\\ndepends on the body shape parameters~\\x00 as in Dyna.\\nDynamic blendshapes are then predicted using\\nBD(~\\x00t, ~\\x00; D)= µD + Df(~\\x00t, ~\\x00) (17)\\nanalogous to Eq. (22) in [Pons-Moll et al. 2015] wheref(· ) is a\\nfunction that takes as input a dynamic control vector,~\\x00t, and pre-\\ndicts the vector of dynamic shape coefﬁcients,~\\x00t. This formula-\\ntion of soft-tissue displacements in terms of dynamic blend shapes\\nmeans that, unlike Dyna, our model remains compatible with cur-\\nrent graphics software. To animate the model, we only need a script\\nto compute the coefﬁcients,~\\x00t = f(~\\x00t, ~\\x00), from the pose sequence\\nand body shape. We observe that the DMPL model produces soft-\\ntissue dynamics that appear more realistic than those of Dyna. See\\nthe Supplemental Videofor visualizations of the training data, dy-\\nnamic blend shapes, and resulting animations.\\n7 Discussion\\nWhy does it work?First, good quality training data is important.\\nHere we use thousands of high-quality registered template meshes.\\nImportantly, the pose training data spans a range of body shapes\\nenabling us to learn a good predictor of joint locations. Second,\\ntraining all the parameters (template shape, blend weights, joint re-\\ngressor, shape/pose/dynamic blend shapes) to minimize vertex re-\\nconstruction error is important to obtain a good model. Here the\\nsimplicity of the model is an advantage as it enables training every-\\nthing with large amounts of data.\\nIn contrast to the scattered-data interpolation methods, we learn\\nthe blend shapes from a large set of training meshes covering the\\nspace of possible poses and learn a simpler function relating pose\\nto blend-shape weights. In particular, our function is linear in the\\nelements of the part rotation matrices. The larger support of the\\nlearned linear functions as opposed to radial basis functions allows\\nthe model to generalize to arbitrary poses; in addition the simple\\nlinear form makes it fast to animate in a game engine without bak-\\ning in the weights. Because elements of a rotation matrix are con-\\nstrained, the model cannot “blow up” when generalizing outside the\\ntraining set.\\nSMPL is an additive model in vertex space. In contrast, while\\nSCAPE also factors deformations into shape and pose deforma-\\ntions, SCAPE multiplies the triangle deformations. With SCAPE a\\nbigger person will have bigger pose-dependent deformations even\\nthough these deformations are not learned for different body shapes.\\nDespite this, in our experiments, the SCAPE approach is less accu-\\nrate at generalizing to new shapes. Ideally one would have enough\\npose data from enough different people to learn a true body-shape-\\ndependent pose deformation space. Our work with DMPL, where\\ndeformations depend on body shape, suggests that this is possible.\\nWhy is it more accurate than BlendSCAPE?Models based on the\\nstatistics of triangle deformations have dominated the recent liter-\\nature [Anguelov et al. 2005; Chen et al. 2013; Freifeld and Black\\n2012; Hasler et al. 2009]. Such models are not trained to repro-\\nduce their training registrations directly. Instead, they are trained'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='Figure 15: DMPL model of soft-tissue motion.Above, two frames of a “running” sequence of a male subject from the Dyna dataset, below\\ntwo frames of a “jumping jacks” sequence of a female subject from the Dyna dataset. From left to right: SMPL, DMPL, and the dynamic\\nblend shapes added to the base body shape. While SMPL models deformations due to pose well it does not model dynamic deformations.\\nDMPL predicts dynamic deformations from motion and body shape, resulting in more life like animations.\\nto reproduce the local deformations that produced those registra-\\ntions. Part of the tractability of training these models comes from\\nthe ability to train deformations independently across triangles. As\\na result, long range distances and relationships are not preserved as\\nwell as local relationships between vertices. We speculate that an\\nadvantage of vertex based models (such as SMPL and [Allen et al.\\n2006]) is that they can be trained to minimize the mean squared\\nerror between the model and training vertices. Theoretically one\\ncould train a SCAPE model to minimize vertex error in global co-\\nordinates, but the inner loop of the optimization would involve solv-\\ning a least-squares problem to reconstruct vertices from the defor-\\nmations. This would signiﬁcantly increase the cost of optimization\\nand make it difﬁcult to train the model with large amounts of data.\\nWhy has it not been done before?While we think the SMPL model\\nis a natural way to extend blend skinning, we are unaware of any\\nprevious published versions. Unfortunately, the obvious implemen-\\ntation makes the pose blend shapes a linear function of~✓. This\\ndoes not work. The key to SMPL’s performance is to make the\\nblendshapes a linear function of the elements ofR⇤(~✓). This for-\\nmulation, sufﬁcient training data, and a good optimization strategy\\nmake it possible to learn the model.\\nThe closest work to ours is the pioneering work of Allen et\\nal. [2006]. Their model is more complex than ours, using radial\\nbasis functions for scattered data interpolation, shape-dependent\\npose deformations, and a ﬁxed set of carrying angles. Consequently\\ntraining it is also complex and requires a good initialization. They\\nhad limited data and difﬁculty with overﬁtting so they restricted\\ntheir body shape PCA space. As a result, the model did not gener-\\nalize well to new shapes and poses. Our simpler model lets us learn\\nit from large datasets and having more data makes the simple model\\n(a) Euler-angles\\n (b) SMPL parameterization\\nFigure 16: Parameterizing pose blend shapes.(a) Pose blend\\nshapes parameterized by Euler angles cause signiﬁcant problems.\\n(b) our proposed parameterization allows the head to rotate in ei-\\nther direction with natural deformations.\\nperform well.\\nOther features for driving pose blend shapes.We experimented\\nwith driving pose blendshapes linearly from other features, such as\\nraw ~✓, simple polynomials of~✓, and trigonometric functions (sin,\\ncos) of~✓. None of these performed as well as our proposed formu-\\nlation. Using raw~✓ has serious limitations because the values vary\\nbetween -⇡ and ⇡. Imagine a twist of the neck (Fig. 16), which\\nproduces negative and postive angles about the vertical axis. Stan-\\ndard LBS will cause the neck to shrink as it rotates in either direc-'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='tion. To counteract this, we need a blend shape that increases the\\nneck volume no matter which direction it rotates. Unfortunately,\\nif the blendshapes are trained to expand during rightwards rotation\\n(to counteract LBS shrinkage), they would contract during leftward\\nrotation.\\nIn general one can think of replacing the raw rotations with any\\nfunctions of rotations and using these to weight the blend shapes.\\nAn exhaustive search is impossible and other features may work\\nas well as our method; for example, we did not experiment with\\nnormalized quaternions.\\nOur pose blend shapes function is also very different from scattered\\ndata interpolation methods like WPSD [Kurihara and Miyata 2004;\\nRhee et al. 2006], which use a discrete number of poses and asso-\\nciated corrections are interpolated between them using RBFs. In\\npractice, a large number of poses might be needed to cover the pose\\nspace well. This makes animation slow since the closest key poses\\nhave to be found at run time.\\nLimitations. The pose-dependent offsets of SMPL are not depen-\\ndent on body shape. It is surprising how well SMPL works without\\nthis, but the general approach would likely not work if we were to\\nmodel a space of nonrealistic animated characters in which body\\npart scales vary widely, or a space of humans that includes infants\\nand adults.\\nThis limitation could be addressed by training a more general func-\\ntion that would take elements ofR⇤(~✓) together with ~\\x00 to predict\\nthe blend shape coefﬁcients. Note that the dynamic blend shape\\ncoefﬁcients do depend on body shape and therefore it should be\\npossible to do the same for the pose blend shapes. This would not\\nsigniﬁcantly complicate the model or run-time behavior but might\\nrequire more training data.\\nAs described, the SMPL model is a function of joint angles and\\nshape parameters only: it does not model breathing, facial motion,\\nmuscle tension, or any changes independent of skeletal joint angles\\nand overall shape. These could potentially be learned as additional\\nadditive blendshapes (as with DMPL) if the appropriate factored\\ndata is available (cf. [Tsoli et al. 2014]).\\nWhile we learn most model parameters, we do not learn them all.\\nWe manually deﬁne the segmentation of the template into parts,\\nthe topology of the mesh, and the zero pose. Theoretically these\\ncould also be learned but we expect only marginal improvements\\nfor signiﬁcant effort.\\nFuture work.SMPL uses 207 pose blend shapes. This could likely\\nbe reduced by performing PCA on the blend shapes. This would re-\\nduce the number of multiplications and consequently increase ren-\\ndering speed. Also, our dynamic model uses PCA to learn the dy-\\nnamic blend shapes but we could learn the elements of these blend\\nshapes directly as we do for the pose blend shapes. Finally, here\\nwe ﬁt our model to registered meshes but could ﬁt SMPL to mocap\\nmarker data (cf. MoSh [Loper et al. 2014]), depth data, or video.\\nWe anticipate that optimizing the pose and shape of a SMPL-LBS\\nmodel will be signiﬁcantly faster than optimizing a SCAPE model\\nof similar quality.\\n8 Conclusions\\nOur goal was to create a skeletally-driven human body model that\\ncould capture body shape and pose variation as well as, or better\\nthan, the best previous models while being compatible with ex-\\nisting graphics pipelines and software. To that end, SMPL uses\\nstandard skinning equations and deﬁnes body shape and pose blend\\nshapes that modify the base mesh. We train the model on thousands\\nof aligned scans of different people in different poses. The form\\nof the model makes it possible to learn the parameters from large\\namounts of data while directly minimizing vertex reconstruction er-\\nror. Speciﬁcally we learn the rest template, joint regressor, body\\nshape model, pose blend shapes, and dynamic blend shapes. The\\nsurprising result is that, when BlendSCAPE and SMPL are trained\\non exactly the same data, the vertex-based model is more accu-\\nrate and signiﬁcantly more efﬁcient to render than the deformation-\\nbased model. Also surprising is that a relatively small set of learned\\nblend shapes do as good a job of correcting the errors of LBS as they\\ndo for DQBS. Using 4D registered meshes we extended SMPL to\\nmodel dynamic soft-tissue deformations as a function of poses over\\ntime using an autoregressive model. SMPL can be exported as an\\nFBX ﬁle and we make scripts available to animate the model in\\ncommon rendering systems. This will allow anyone to realistically\\nanimate human bodies.\\n9 Acknowledgements\\nWe thank F. Bogo for help with registration and 3D body model-\\ning; B. Allen and B. Curless for information about their model;\\nB. Mohler, J. Tesch, and N. Troje for technical discussion;\\nA. Keller, E. Holderness, S. Polikovsky, and S. Streuber for help\\nwith data acquisition; J. Anning for voice recording and technical\\nsupport; and A. Quiros Ramirez for web development.\\nConﬂict-of-Interest Disclosure: MJB is a founder, shareholder,\\nand member of the board of Body Labs Inc., which is commercial-\\nizing body shape technology.\\nReferences\\nALLEN ,B . ,CURLESS ,B . ,AND POPOVI ´C, Z. 2002. Articulated\\nbody deformation from range scan data.ACM Trans. Graph.\\n(Proc. SIGGRAPH) 21, 3 (July), 612–619.\\nALLEN ,B . ,CURLESS ,B . ,AND POPOVI ´C, Z. 2003. The space of\\nhuman body shapes: Reconstruction and parameterization from\\nrange scans.ACM Trans. Graph. (Proc. SIGGRAPH) 22, 3, 587–\\n594.\\nALLEN ,B . ,C URLESS ,B . ,P OPOVI ´C,Z . , AND HERTZMANN ,\\nA. 2006. Learning a correlated model of identity and pose-\\ndependent body shape variation for real-time synthesis. InPro-\\nceedings of the 2006 ACM SIGGRAPH/Eurographics Sympo-\\nsium on Computer Animation, Eurographics Association, Aire-\\nla-Ville, Switzerland, Switzerland, SCA ’06, 147–156.\\nANGUELOV ,D . ,S RINIV ASAN,P . ,K OLLER ,D . ,T HRUN ,S . ,\\nRODGERS ,J . ,AND DAV I S, J. 2005. SCAPE: Shape Comple-\\ntion and Animation of PEople.ACM Trans. Graph. (Proc. SIG-\\nGRAPH 24, 3, 408–416.\\nBARAN ,I . ,AND POPOVI ´C, J. 2007. Automatic rigging and anima-\\ntion of 3D characters.ACM Trans. Graph. (Proc. SIGGRAPH)\\n26, 3 (July).\\nBOGO ,F . ,ROMERO ,J . ,LOPER ,M . ,AND BLACK , M. J. 2014.\\nFAUST: Dataset and evaluation for 3D mesh registration. In\\nProc. IEEE Conf. on Computer Vision and Pattern Recognition\\n(CVPR), 3794 –3801.\\nCHANG ,W . ,AND ZWICKER , M. 2009. Range scan registration\\nusing reduced deformable models.Computer Graphics Forum\\n28, 2, 447–456.\\nCHEN ,Y . ,LIU,Z . ,AND ZHANG , Z. 2013. Tensor-based human\\nbody modeling. InIEEE Conf. on Computer Vision and Pattern\\nRecognition (CVPR), 105–112.'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='2000. CMU graphics lab motion capture database. http://\\nmocap.cs.cmu.edu. Accessed: 2012-12-11.\\nCORAZZA ,S . ,AND GAMBARETTO , E., 2014. Automatic gener-\\nation of 3D character animation from 3D meshes, Aug. 5. US\\nPatent 8,797,328.\\nDE AGUIAR ,E . ,THEOBALT ,C . ,THRUN ,S . ,AND SEIDEL ,H . - P .\\n2008. Automatic conversion of mesh animations into skeleton-\\nbased animations.Computer Graphics Forum 27, 2, 389–397.\\n2015. Dyna dataset. http://dyna.is.tue.mpg.de/. Ac-\\ncessed: 2015-05-15.\\nFREIFELD ,O . ,AND BLACK , M. J. 2012. Lie bodies: A mani-\\nfold representation of 3D human shape. InEuropean Conf. on\\nComputer Vision (ECCV), Springer-Verlag, A. Fitzgibbon et al.\\n(Eds.), Ed., Part I, LNCS 7572, 1–14.\\nHASLER ,N . ,STOLL ,C . ,SUNKEL ,M . ,ROSENHAHN ,B . ,AND\\nSEIDEL , H. 2009. A statistical model of human pose and body\\nshape. Computer Graphics Forum 28, 2, 337–346.\\nHASLER ,N . ,THORM ¨AHLEN ,T . ,ROSENHAHN ,B . ,AND SEIDEL ,\\nH.-P. 2010. Learning skeletons for shape and pose. InProceed-\\nings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D\\nGraphics and Games, ACM, New York, NY , USA, I3D ’10, 23–\\n30.\\nHIRSHBERG ,D . ,LOPER ,M . ,RACHLIN ,E . ,AND BLACK ,M .\\n2012. Coregistration: Simultaneous alignment and modeling of\\narticulated 3D shape. In European Conf. on Computer Vision\\n(ECCV), Springer-Verlag, A. F. et al. (Eds.), Ed., LNCS 7577,\\nPart IV , 242–255.\\nJAMES ,D .L . ,AND TWIGG , C. D. 2005. Skinning mesh anima-\\ntions. ACM Trans. Graph. 24, 3 (July), 399–407.\\nKAVA N,L . ,AND ˇZ ´ARA , J. 2005. Spherical blend skinning: A\\nreal-time deformation of articulated models. InProceedings of\\nthe 2005 Symposium on Interactive 3D Graphics and Games,\\nACM, New York, NY , USA, I3D ’05, 9–16.\\nKAVA N,L . ,COLLINS ,S . ,ˇZ ´ARA ,J . ,AND O’SULLIV AN, C. 2008.\\nGeometric skinning with approximate dual quaternion blending.\\nACM Transactions on Graphics (TOG) 27, 4, 105:1–105:23.\\nKAVA N,L . ,COLLINS ,S . ,AND O’SULLIV AN, C. 2009. Auto-\\nmatic linearization of nonlinear skinning. InProceedings of the\\n2009 Symposium on Interactive 3D Graphics and Games, ACM,\\nNew York, NY , USA, I3D ’09, 49–56.\\nKRY,P .G . ,JAMES ,D .L . ,AND PAI, D. K. 2002. EigenSkin:\\nReal time large deformation character skinning in hardware. In\\nProceedings of the 2002 ACM SIGGRAPH/Eurographics Sym-\\nposium on Computer Animation, ACM, New York, NY , USA,\\nSCA ’02, 153–159.\\nKURIHARA ,T . ,AND MIYATA, N. 2004. Modeling deformable\\nhuman hands from medical images. InProceedings of the 2004\\nACM SIGGRAPH/Eurographics Symposium on Computer An-\\nimation, Eurographics Association, Aire-la-Ville, Switzerland,\\nSwitzerland, SCA ’04, 355–363.\\nLAWSON ,C .L . ,AND HANSON , R. J. 1995.Solving least squares\\nproblems. Classics in applied mathematics. SIAM, Philadelphia,\\nPA. SIAM : Society of industrial and applied mathematics.\\nLE,B .H . ,AND DENG , Z. 2012. Smooth skinning decomposi-\\ntion with rigid bones.ACM Trans. Graph. 31, 6 (Nov.), 199:1–\\n199:10.\\nLE,B .H . ,AND DENG , Z. 2014. Robust and accurate skeletal\\nrigging from mesh sequences.ACM Trans. Graph. 33, 4 (July),\\n84:1–84:10.\\nLEWIS ,J .P . ,C ORDNER ,M . , AND FONG , N. 2000. Pose\\nspace deformation: A uniﬁed approach to shape interpolation\\nand skeleton-driven deformation. InProceedings of the 27th An-\\nnual Conference on Computer Graphics and Interactive Tech-\\nniques, ACM Press/Addison-Wesley Publishing Co., New York,\\nNY , USA, SIGGRAPH ’00, 165–172.\\nLOPER ,M .M . ,AND BLACK , M. J. 2014. OpenDR: An ap-\\nproximate differentiable renderer. InComputer Vision – ECCV\\n2014, Springer, Heidelberg, D. Fleet, T. Pajdla, B. Schiele, and\\nT. Tuytelaars, Eds., vol. 8695 ofLecture Notes in Computer Sci-\\nence, 154–169.\\nLOPER ,M .M . ,MAHMOOD ,N . ,AND BLACK , M. J. 2014. MoSh:\\nMotion and shape capture from sparse markers.ACM Trans.\\nGraph., (Proc. SIGGRAPH Asia) 33, 6 (Nov.), 220:1–220:13.\\nMERRY,B . ,MARAIS ,P . ,AND GAIN , J. 2006. Animation space:\\nA truly linear framework for character animation.ACM Trans.\\nGraph. 25, 4 (Oct.), 1400–1423.\\nMILLER ,C . ,ARIKAN ,O . ,AND FUSSELL , D. 2010. Franken-\\nrigs: Building character rigs from multiple sources. InProceed-\\nings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D\\nGraphics and Games, ACM, New York, NY , USA, I3D ’10, 31–\\n38.\\nMOHR ,A . ,AND GLEICHER , M. 2003. Building efﬁcient, accu-\\nrate character skins from examples.ACM Trans. Graph. (Proc.\\nSIGGRAPH), 562–568.\\nNOCEDAL ,J . ,AND WRIGHT , S. J. 2006.Numerical Optimization,\\n2nd ed. Springer, New York.\\nPONS -MOLL ,G . ,ROMERO ,J . ,MAHMOOD ,N . ,AND BLACK ,\\nM. J. 2015. Dyna: A model of dynamic human shape in mo-\\ntion. ACM Transactions on Graphics, (Proc. SIGGRAPH) 34,4\\n(July), 120:1–120:14.\\nRHEE ,T . ,LEWIS ,J . ,AND NEUMANN , U. 2006. Real-time\\nweighted pose-space deformation on the GPU.EUROGRAPH-\\nICS 25, 3.\\nROBINETTE ,K . ,BLACKWELL ,S . ,DAANEN ,H . ,BOEHMER ,M . ,\\nFLEMING ,S . ,BRILL ,T . ,HOEFERLIN ,D . ,AND BURNSIDES ,\\nD. 2002. Civilian American and European Surface Anthropom-\\netry Resource (CAESAR) ﬁnal report. Tech. Rep. AFRL-HE-\\nWP-TR-2002-0169, US Air Force Research Laboratory.\\nROUET ,C . ,AND LEWIS , J., 1999. Method and apparatus for cre-\\nating lifelike digital representations of computer animated ob-\\njects by providing corrective enveloping, Mar. 16. US Patent\\n5,883,638.\\nSCHAEFER ,S . ,AND YUKSEL , C. 2007. Example-based skele-\\nton extraction. InProceedings of the Fifth Eurographics Sympo-\\nsium on Geometry Processing, Eurographics Association, Aire-\\nla-Ville, Switzerland, Switzerland, SGP ’07, 153–162.\\nSEO,H . ,C ORDIER ,F . , AND MAGNENAT -THALMANN ,N .\\n2003. Synthesizing animatable body models with parameter-\\nized shape modiﬁcations. InProceedings of the 2003 ACM SIG-\\nGRAPH/Eurographics Symposium on Computer Animation, Eu-\\nrographics Association, Aire-la-Ville, Switzerland, Switzerland,\\nSCA ’03, 120–125.\\nTSOLI ,A . ,MAHMOOD ,N . ,AND BLACK , M. J. 2014. Breathing\\nlife into shape: Capturing, modeling and animating 3D human'), Document(metadata={'producer': 'Mac OS X 10.10.5 Quartz PDFContext', 'creator': 'TeX', 'creationdate': \"D:20150917102908Z00'00'\", 'moddate': \"D:20150917102908Z00'00'\", 'source': 'docs/SMPL2015.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='breathing. ACM Trans. Graph., (Proc. SIGGRAPH) 33, 4 (July),\\n52:1–52:11.\\nWANG ,X .C . ,AND PHILLIPS , C. 2002. Multi-weight enveloping:\\nLeast-squares approximation techniques for skin animation. In\\nProceedings of the 2002 ACM SIGGRAPH/Eurographics Sym-\\nposium on Computer Animation, ACM, New York, NY , USA,\\nSCA ’02, 129–138.\\nWANG ,R .Y . ,PULLI ,K . ,AND POPOVI ´C, J. 2007. Real-time\\nenveloping with rotational regression.ACM Trans. Graph. (Proc.\\nSIGGRAPH) 26, 3 (July).\\nWEBER ,O . ,S ORKINE ,O . ,L IPMAN ,Y . ,AND GOTSMAN ,C .\\n2007. Context-aware skeletal shape deformation. Computer\\nGraphics Forum 26, 3 (Sept.), 265–274.\\nA Appendix\\nA.1 Mathematical Notation\\nWe summarize our notation here and in Table 1. MatricesA 2\\nRn⇥m are denoted with math calligraphic typeface. VectorsA 2\\nRm are denoted with uppercase boldface, expect for the special\\ncase of 3-vectors, which are denoted with lower casea 2 R3 to\\ndistinguish a particular vertex from a vector of concatenated ver-\\ntices. The notation A() : Rm 7! Rn is used to denote a func-\\ntion that maps vectors in anm-dimensional space to vectors inn-\\ndimensional space. Typically, indices are used as follows:j iterates\\nover mesh registrations,k iterates over joint angles andi iterates\\nover subjects, andt denotes time.\\nTable 1:Table of Notation\\nModel generation functions\\nW , Skinning function\\nM , SMPL function\\nBP , Pose blendshapes function\\nBS , Shape blendshapes function\\nBD , Dynamic blendshapes function\\nJ , Joint regressor: Predicts joints from surface\\nModel input parameters (controls)\\n~\\x00 , Shape parameters\\n~✓ , Pose parameters\\n~! , Scaled axis of rotation; the 3 pose parameters cor-\\nresponding to a particular joint\\n~\\x00 , Dynamic control vector\\n~\\x00 , Dynamic shape coefﬁcients\\n~✓⇤ , Zero pose or rest pose; the effect of the pose blend-\\nshapes is zero for that pose\\nModel parameters (parameters learned)\\nS , Shape blendshapes\\nP , Pose blendshapes\\nW , Blendweights\\nJ , Joint regressor matrix\\n¯T , Mean shape of the template\\nTraining data\\nV , A registration\\nVP , Pose dataset registration\\nVS , Shape dataset registration\\nˆTP , Pose dataset subject shape; body vertices in the tem-\\nplate pose\\nˆJP , Pose dataset subject joint locations in the template\\npose\\nˆTP\\nµ , Mean shape of a pose subject; body vertices in the\\ntemplate pose\\nˆTS , Shape dataset subject shape; body vertices in the\\ntemplate pose\\nˆTS\\nµ , Mean shape of a subject in the shape dataset; body\\nvertices in the template pose'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 1\\nSMPLitex: A Generative Model and Dataset\\nfor 3D Human Texture Estimation from\\nSingle Image\\nDan Casas\\ndan.casas@urjc.es\\nMarc Comino-Trinidad\\nmarc.comino@urjc.es\\nUniversidad Rey Juan Carlos\\nMadrid, Spain\\nAbstract\\nWe propose SMPLitex, a method for estimating and manipulating the complete 3D\\nappearance of humans captured from a single image. SMPLitex builds upon the recently\\nproposed generative models for 2D images, and extends their use to the 3D domain\\nthrough pixel-to-surface correspondences computed on the input image. To this end, we\\nfirst train a generative model for complete 3D human appearance, and then fit it into the\\ninput image by conditioning the generative model to the visible parts of subject. Fur-\\nthermore, we propose a new dataset of high-quality human textures built by sampling\\nSMPLitex conditioned on subject descriptions and images. We quantitatively and qual-\\nitatively evaluate our method in 3 publicly available datasets, demonstrating that SM-\\nPLitex significantly outperforms existing methods for human texture estimation while\\nallowing for a wider variety of tasks such as editing, synthesis, and manipulation.\\ndancasas.github.io/projects/SMPLitex\\n1 Introduction\\nFigure 1: From a single image where a human is partly visible (left), SMPLitex automatically\\nestimates a complete 3D texture map that can be applied to SMPL [34] body mesh sequences.\\nCreating photorealistic 3D virtual humans is a long-standing goal in Computer Graphics\\nand Computer Vision, with important applications in many areas including telecommuni-\\ncations, entertainment, online shopping, and medicine. Among the many tasks required to\\nproduce digital humans, appearance synthesis is the fundamental step to achieving photo-\\nrealism. To model virtual humans at scale, methods to create realistic-looking 3D human\\ntextures are needed.\\n© 2023. The copyright of this document resides with its authors.\\nIt may be distributed unchanged freely in print or electronic forms.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='2 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\nIdeally, a method to model human appearance should be generative (to create new com-\\nplete textures), easy to fit into partial observations (to recover textures from images), and\\ncompatible with traditional animation pipelines (to use the textures in popular commercial\\ntools). Unfortunately, despite the impressive advances for digital humans, existing mod-\\nels for human synthesis typically focus on solving one of these challenges in isolation.\\nFor example, recent methods have shown impressive results in posed image synthesis tasks\\n[9, 12, 30, 36, 45, 52, 53] using generative strategies such as GANs [10, 26] or V AEs, guided\\nwith 2D pose representations or text [24]. However, these methods directly output camera-\\nspace 2D images of humans but do not generate a complete texture map. This precludes their\\nuse in standard 3D animation pipelines, where UV texture maps for texturing 3D meshes are\\nused. Additionally, they are usually not ready to be used to recover textures from in-the-wild\\nimages. Similarly, other works leverage neural rendering pipelines [23, 41, 42, 44, 58] to\\ngenerate view-dependent posed avatars but do not output 3D texture maps either.\\nAlternatively, and closer to ours, other works focus on complete texture estimation from\\nsingle image [22, 25, 29, 40, 59, 63] and are able to recover 3D texture maps from ca-\\nsual images. Most of these methods used CNN architectures [22, 29] to infer the com-\\nplete 3D texture map from a single image, sometimes using multi-view supervision [63] or\\ntransformer-based architectures [59]. However, they are limited to generating textures from\\nimages, which precludes their use in applications that require the synthesis of unseen virtual\\nhumans (e.g., via text prompts). Most importantly, they typically output low-detail textures\\ndue to the limited expressivity of the latent space of the network. Additionally, we argue that\\nthese undesired properties prevent the use of existing methods to build large and high-quality\\npublic datasets of 3D human textures which, we believe, is a major shortfall in the field.\\nIn this paper, we address these limitations and propose SMPLitex, a generative method\\nfor complete 3D human texture synthesis. SMPLitex enables the estimation of 3D human\\ntextures from single images that can be directly applied to SMPL meshes as shown in Figure\\n1. Additionally, since it is a generative method, it also allows for the synthesis of new\\ntextures via text prompts or image editing, as shown in Figures 2 and 3. Under the hood,\\nSMPLitex leverages the recently proposed diffusion models for 2D image synthesis [47, 49]\\nbuilt from a hierarchy of denoising processes. Despite the impressive results of such models\\nin 2D tasks [8, 17, 18, 35, 56], extending diffusion models to 3D humans requires addressing\\ntwo important challenges: spatial regularization, to enforce multi-view consistency; and 3D\\nawareness, to enable 2D-to-3D tasks. Our key observation is that, for human-related tasks, a\\nproxy of the 3D geometry visible in any image is coarsely modeled with human body models\\nsuch as SMPL [34], which opens the door to 2D diffusion models for 3D humans. To this\\nend, we first learn a domain-specific diffusion model that is trained to generate unwrapped\\n3D textures of humans, which implicitly learns multi-view consistency. Then, to provide\\nthe 3D awareness required for 2D-to-3D tasks such as 3D texture from monocular input,\\nwe estimate pixel-to-surface correspondences [13] to project image pixels to an incomplete\\n3D texture map. Leveraging the 2D structure implicitly enforced in our domain-specific\\ndiffusion model, we are able to inpaint the incomplete 3D texture map.\\nWe demonstrate that the proposed model, SMPLitex, outperforms state-of-the-art meth-\\nods [29, 59] for 3D human texture estimation in 3 publicly available datasets [24, 33, 64],\\ngenerating high-resolution texture maps. Additionally, we exploit the generative capabili-\\nties of the proposed model to create a new dataset of high-quality 3D textures. Our new\\ndataset overcomes the quality, diversity, and number of samples of existing datasets such as\\nSURREAL [55]. This paves the way for new data-driven models that require photorealistic\\nhuman data such as 3D pose estimation, 3D human reconstruction, and neural rendering.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 3\\nIn summary, our main contributions are:\\n• SMPLitex, a new generative model for 3D human textures that can be used as a drop-in\\nreplacement for textures in any SMPL-based pipeline.\\n• A novel diffusion-based method to infer 3D human textures from single RGB input.\\n• A new dataset of high-quality 3D human textures that significantly surpasses the detail,\\ndiversity, and size of existing datasets.\\n2 Related Work\\nTexture recovery from single image.This group of methods attempts to recover the com-\\nplete appearance of humans [3, 5, 22, 29, 40, 59, 63], faces [51] or category-specific objects\\n[25, 38] from a single image. To tackle the challenges arising from such ill-posed prob-\\nlem, a variety of learning-based solutions relying on neural networks have been proposed,\\nincluding the use of multi-view supervision [40, 63], transformer-based architectures [59],\\nand differentiable rendering [22, 25].\\nSimilar to ours, some works frame the texture recovery problem as image inpainting\\nproblem [29, 40]. For example, Lazova et al. [29] computes pixel-to-surface correspon-\\ndences from the input image and project the pixel information into a partial texture map. The\\nincomplete texture is then inpainted using a GAN-based network, generating a complete 3D\\ntexture of the input image. Neverovaet al. [40] also train a partial texture inpainting network\\nto infer the occluded parts of the body, which is supervised by multi-view ground truth data.\\nInstead of inpainting a texture map, other methods directly predict the full texture given the\\ninput image [22, 59, 60, 63]. For example, Xu et al. [59] map 3D coordinates of a human\\nbody mesh to a UV texture map which, in combination with a 2D part segmentation image,\\nis converted to a texture map using a transformer-based network. Wang et al. [22] utilize\\na distance metric based on a re-identification loss to learn to generate texture maps given\\na dataset of images of humans taken from different viewpoints and their corresponding 3D\\npose. Similarly, Zhao et al. [63] add part-based segmentation and enforce cross-view con-\\nsistency to learn to generate complete texture maps. Despite the impressive results, these\\nmethods typically output low-detail textures due to the limited expressivity of the latent\\nspace of the networks used. Furthermore, they are also limited to generating textures from\\nimages, which precludes their use in applications that require the synthesis of new textures\\nor their manipulation, for example, via text prompts.\\nClosely related to the texture estimation task are the method that aim to reconstruct 3D\\nhumans and appearance from single image [3, 15, 31, 39, 65]. These methods produce high-\\nfidelity 3D reconstructions, including fine geometric details, but the estimated appearance\\nis usually not explicitly baked into a consistent UV texturemap. Furthermore, they do not\\nenable the synthesis of unseen appearances.\\nPosed image synthesis.Instead of predicting the complete 3D texture map, this group of\\nmethods aim at generating images of posed humans [9, 11, 12, 24, 30, 36, 45, 52, 53], hence\\nonly requiring to synthesize the visible parts of the subject. Under the hood, these methods\\nuse generative strategies, such as GANs [10, 26] or V AEs, guided with 2D pose representa-\\ntions [45], dense surface correspondences [1, 11], or text prompts [24] to describe the target\\npose. For example, Sarkar et al. [52] encode the partial ( i.e., visible) UV-space appearance'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='4 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\nto a global latent vector to modulate a StyleGAN2 [26] image generator. Similarly, AlBahar\\net al. [1] inpaint a correspondence field and transfer local surface details to the target pose.\\nDespite the high-quality results of these methods, most of them do not output an explicit\\ntexture map that can be used in 3D animation pipelines. This limits their use to purely 2D\\nimage synthesis use cases. [14, 32] are notable exceptions and predict explicit dynamic\\ntexture maps of the subject for pose synthesis, but require per-subject retraining.\\nAvatars from text.Also related to ours are the methods that aim at synthesizing humans\\nfrom text descriptions [7, 19, 24, 28, 62]. These methods combine recent image generative\\nmodels [9] based on GANs, V AEs, or diffusion models, with large vision-language pre-\\ntrained models such as CLIP [46] to condition the output. For example, AvatarCLIP [19] is\\nable to generate and animate 3D textured humans directly from text, and Text2Human [24]\\nsynthesizes high-quality 2D posed humans given detailed outfit descriptions. Other works\\nfocus on more general objects [21, 27, 37]. However, they do not generate consistent texture\\nmaps, and cannot easily be fitted into partial observations to estimate textures from images.\\n3 3D Human Texture Estimation\\nGiven an input RGB image x, where a person is visible or partially visible, our goal is to\\nestimate a 3D texture map u that encodes the complete appearance of the subject. This is a\\nchallenging ill-posed problem because (1) the 3D geometry of the scene (i.e., the 3D surface\\nof the person) is unknown; and (2) many parts of the image suffer from natural occlusions\\nand self-occlusions (i.e., not all surface points are visible).\\nTo address this problem, a common approach [29, 59] is, first, to use a coarse geome-\\ntry proxy [34] and infer pixel-to-surface correspondences [13] to build an incomplete texture\\nmap; and later use an image-to-image translation framework to inpaint or estimate the incom-\\nplete texture. However, the limited expressivity of existing methods leads to low-resolution\\ninpainted textures that lack detail. Furthermore, the resulting models cannot be manipulated\\nwith text prompts, which limits their applicability to image-to-image tasks.\\nIn contrast, we propose a pipeline based on image diffusion models [49] that is capa-\\nble of recovering high-quality textures, and additionally, it naturally allows for text-based\\nmanipulations. Figure 2 presents a visualization of our pipeline. In the rest of this section,\\nwe first describe how we formulate and train our generative model (Section 3.1), and then\\ndescribe how we leverage it to estimate complete 3D textures from in-the-wild monocular\\nRGB images (Section 3.2).\\n3.1 Generative 3D Human Textures\\n3.1.1 Background\\nSMPLitex uses a diffusion model as a generative backbone. Diffusion models are a type of\\ngenerative model that gradually remove noise from an image to learn the distribution space\\npθ [16]. To accomplish this, the model starts with a sampled Gaussian noise and performs a\\nstep-by-step denoising process over T time steps until it produces a final, noise-free image.\\nDuring each step of the denoising process, the diffusion model generates noiseεt that is used\\nto create an intermediate denoised image xt . The initial noise xT corresponds to the final\\nimage, while the fully denoised image x0 corresponds to the starting image. This denoising'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 5\\nmasked\\npixel-to surface\\n“With sunglasses \\nand shorts”\\nmask\\ndiffusion model\\npixel-to-surfaceinput image\\n complete texture map\\nufull\\nfront 3D render back 3D renderpartial texture map\\nupart\\nSMPLitex\\ntexturemap samples\\nSMPLitex\\nFigure 2: Overview of the proposed approach for texture estimation from a single image.\\nprocess is typically modeled as a Markov transition probability as follows\\npθ (xT :0) =p(xT )\\n1\\n∏\\nt=T\\npθ (xt−1|xt ) (1)\\nTo improve the efficiency of such diffusion models,latent diffusion models (LDM) [49]\\noperate on a lower dimensional latent space z pre-trained using a variational autoencoder.\\nDuring training, for an image x, noise is added to the encoded image z = E(x), where E\\nis the pre-trained encoder, leading to zt where the noise level increases with t. Analogous\\nto the original diffusion model, the LDM process can be seen as a sequence of denoising\\nmodels with shared parameters θ that learn to predict a noisy image εθ (zt , c, t), where t is\\nthe timestep and c a text condition. LDMs are trained by minimizing the loss term\\nEzt ∈E(x), t, c, ε∼N(0,1)[|ε −εθ (zt , c, t)|2\\n2] (2)\\nOnce trained, the LDM model can generate new samples following the diffusion process\\nin reverse mode, iteratively predicting the noise to be removed from a randomly sampled\\nGaussian noise (potentially conditioned on text and time step).\\n3.1.2 Diffusion Model for 3D Humans\\nMany extensions of the original LDM [49] model have been recently proposed for a wide\\nvariety of tasks and data modalities, including image inpainting [8, 35, 54] and video gener-\\nation [17, 18, 56]. In this work, we look into how LDM can be adapted to the specific case\\nof 3D human appearance. However, naively extending LDMs to 3D domains is challenging\\nsince multi-view consistency cannot be guaranteed. A few recent works demonstrate promis-\\ning results in text-to-3D [43] or novel view synthesis [4, 57] tasks, but articulated objects and\\nhigh-resolution images remain a challenge.\\nOur key observation is that, for the specific case of 3D humans, these limitations can\\nbe circumvented by using a 3D-to-2D parametrization of the surface ( i.e., a UV map of the\\nmesh surface). By working directly on the UV map, we are able to encode the 3D appearance\\nof the human directly on a 2D image, opening the door to the potential of LDMs models for\\nappearance synthesis, inpainting, and manipulation.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='6 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\nTherefore, we propose to leverage the highly expressive LDM proposed by Rombach et\\nal. [49], and fine-tune it to encode UV textures of humans. The main underlying challenge\\nof this task is how to ensure that the expressivity of the original model is preserved ( i.e.,\\nthe fine-tuned model is able to generate detailed and rich images that are not in the fine-\\ntuning training set) while satisfying the spatial constraints inherent in the UV textures ( i.e.,\\nthe 2D-to-3D parameterization). Crucially, recent text-to-image models [20, 50] address an\\nanalogous problem for model personalization, where the goal is to learn to generate images\\nwith a specific style or for a particular subject. Under the hood, these models use a class-\\nspecific prior preservation loss that enables the synthesis of the target subject or style in\\narbitrary scenarios, just using an extremely reduced set of training samples. Inspired by\\nthis, SMPLitex leverages the work of Ruiz et al. [50] to fine-tune the model of Rombach et\\nal. [49] such that it is constrained to synthesize SMPL UV texture maps. In practice, we use\\n10 UV texture maps for SMPL from [2, 29] to fine-tune the model [49] available at [48] for\\n1,500 iterations. At inference time, our results use 50 denoising steps and a classifier free\\nguidance (CGF) of 2.0.\\n3.2 Human Texture Estimation from Single Image\\nThe diffusion model described in Section 3.1 enables the synthesis of high-quality UV tex-\\nture maps of humans. As discussed above, the diffusion model is sampled conditioned to\\nthe time step, such that it removes the noise at time t, but it also allows for additional condi-\\ntioning signals (e.g., condition text c in Equation 2). Our key intuition is that to enable the\\nestimation of 3D human appearance from a single image, we can condition the synthesis of\\nan LDM model for human appearance to the visible parts of the subject in the input image.\\nIn other words, we are interested in fitting SMPLitex into natural images.\\nTo this end, similar to [29], we leverage the fact that we are under the assumption of\\nmodeling 3D humans and use state-of-the-art pixel-to-surface correspondence models to\\ncompute a partial texture map upart. More specifically, given an input image x, we estimate\\npixel-to-surface correspondences d [40] and project the pixels of x with assigned surface\\ncorrespondences to a partial UV map upart, which will be used as a conditional signal.\\nHowever, a naive use of the pixel-to-surface correspondenced can potentially lead to un-\\ndesired partial UV maps since d typically coarsely estimates the foreground silhouette. This\\ncan lead to background pixels projected into the UV map, which significantly degenerates\\nthe condition image. We mitigate this issue by computing an accurate human silhouettes [6],\\nthat we use to mask the pixel-to-surface image d. More formally, we compute our partial\\nUV map as\\nupart = Π(x, d ⊙s) (3)\\nwhere ⊙ is the Hadamard product, Π is the operator that projects all pixels p ∈ x to their\\ncorresponding surface coordinate according to the map d ⊙s.\\nWith the partial texture map upart computed, we can infer the complete texture map ufull\\nof the input image x by just sampling the diffused model described in Section 3.1 usingupart\\nas an additional conditioning signal.\\n4 SMPLitex Dataset\\nTaking advantage of the generative capabilities of the appearance model described in Section\\n3.1, we build a dataset of high-quality textures by simply sampling the latent space. More'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 7\\n\"Casual outfit\" \"Military soldier costume\" \"Race car driver\"\\n\"Pirate costume\" \"Superhero costume\" \"Futuristic astronaut\"\\n\"Hippie outfit\" \"Business outfit\" \"Angela Merkel, shirt\"\\nFigure 3: SMPLitex texture samples, side-by-side to their corresponding front and back\\nSMPL render. Notice that these textures are not used for training SMPLitex, instead, they\\nare part of the dataset that we built by arbitrarily sampling our model. We believe such\\nhigh-quality textures will open the door to new possibilities in the area of virtual humans.\\nspecifically, we use text conditioning prompts describing arbitrary outfit combinations, cos-\\ntumes, sports apparel, job titles, and facial characteristics. Figure 3 depicts 9 samples of the\\ndataset, showcasing a large variety of garment types, outfits, identities, and facial details. In\\ntotal, the SMPLitex dataset consists of 100 curated textures, see the supplementary material\\nfor more details. To clarify, on top of the dataset we will also release the trained SMPLitex\\nmodel to generate new arbitrary samples, and it is the core component of our method to re-\\ncover textures from single images. However, we believe that releasing a fixed set of curated\\ntextures will benefit future research.\\n5 Results and Evaluation\\nWe qualitatively and quantitatively evaluate our method on 3 publicly available datasets [24,\\n59, 61] and demonstrate that our results compare favorably with state-of-the-art methods for\\ntexture estimation. In contrast to existing methods, SMPLitex can deal with both low and\\nhigh-resolution imagery, and it is robust to multi-view consistency metrics.\\nEvaluation on DeepFashion-MultiModal [24].This dataset consists of a large collection\\nof high-resolution fashion images (750 ×1101 pixels), where subjects wear a wide variety\\nof clothing styles. In Figure 4 we present our qualitative results on this dataset, and compare\\nthem to the results of the state-of-the-art closest methods [29, 59]. SMPLitex qualitatively\\noutperforms the method of Lazova et al. [29], which is based on a GAN inpainting network\\nthat is unable to output high-quality garment details that are visible in the input image. In\\ncontrast, SMPLitex textures exhibit fine details such as wrinkles and facial attributes, while\\nalso extrapolating well to the occluded parts of the body.\\nEvaluation on Market-1501 [64].This dataset consists of a large collection of low-resolution\\nimages (64 ×128 pixels) of 1501 different subjects, captured in an urban scenario. It was\\noriginally proposed to evaluate person re-identification tasks, but Xu et al. [59] extended'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='8 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\nInput SMPLitex (ours) Lazovaet al.[29] Xu et al.[59]\\nFigure 4: Qualitative comparison with state-of-the-art methods [29, 59] in the DeepFashion-\\nMultiModal dataset [24]. SMPLitex clearly outperforms the texture quality of previous\\nmethods, recovering fine details such as garment wrinkles and facial attributes.\\ntheir use to evaluate human texture estimation. The idea is to estimate the texture of a sub-\\nject on an image and test the fidelity of the recovered texture by rendering and comparing it\\nto another image of the same subject captured from another viewpoint. We use the same test\\nset and evaluation protocol defined by Xu et al. [59].\\nSSIM ↑ LPIPS ↓\\nCMR [25] 0.7142 0.1275\\nHPBTT [63] 0.7420 0.1168\\nRSTG [22] 0.6735 0.1778\\nTexGlo [60] 0.6658 0.1776\\nTexFormer [59] 0.7422 0.1154\\nSMPLitex (ours) 0.8648 0.0695\\nTable 1: Quantitative evaluation on Market-\\n1501 [64], following the protocol and evalu-\\nation code defined by Xu et al. [59].\\nIn Table 1 we show that our approach SM-\\nPLitex quantitatively outperforms the state-\\nof-the-art method by Xu et al. [59]. Fur-\\nthermore, in Figure 5 we present a qualita-\\ntive comparison that demonstrates that SM-\\nPLitex can infer faithful textures despite the\\nextremely low-resolution image input. Im-\\nportantly, notice that Xu et al. is a method\\ntrained specifically on Market-1501 ( i.e., it\\ndoes not generalize well to other datasets, as\\nwe show above in the evaluation with Deep-\\nFashion). SMPLitex does not suffer from\\nsuch an image-specific domain, while still be-\\ning competitive in the Market-1501 dataset.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 9\\nInput [59] Ours Input [59] Ours Input [59] Ours Input [59] Ours\\nFigure 5: Qualitative comparison on Market-1501 dataset [64]. Despite not being trained\\nfor this challenging dataset of 64 ×128 pixel images, SMPLitex is able to infer convincing\\ntextures that are at least on par with the dataset-specific method of Xu et al. [59].\\nInput\\nimage Ours [59] Ground\\ntruth Ours [59] Ground\\ntruth Ours [59]\\nFigure 6: Multi-view consistency evaluation on THUman2.0 dataset. Using the input image\\non the left, we show output textures by SMPLitex (ours) and [59]. Next, we show 2 validation\\nground truth viewpoints and demonstrate that our renders closely match the ground truth.\\nSSIM ↑ LPIPS ↓\\nTexFormer [59] 0.8761 0.1223\\nSMPLitex (ours) 0.8829 0.1067\\nTable 2: Quantitative evaluation on THU-\\nman2.0 [61].\\nEvaluation on THUman2.0 [61]. To fur-\\nther evaluate our approach, we contribute\\nwith a new evaluation protocol for 3D texture\\ninference from a single image. To this end,\\nwe leverage the THUmans2.0 [61] dataset,\\na high-quality collection of 3D scans with\\nSMPL pose labels, and generate a test set\\nconsisting of a render of each scan from a spe-\\ncific viewpoint. We then compare the estimated texture rendered from a different viewpoint\\nwith ground truth renders of the scan and compute the pixel similarity. Figure 6 presents\\nqualitative results of this evaluation, demonstrating that our estimated textures closely match\\nthe ground truth images from different viewpoints. Since we have 3D ground truth scans, we\\nare able to compute camera-space pixel-based errors of the rendered textures. Table 2 shows\\nthat SMPLitex outperforms the state-of-the-art method of Xu et al [59] in both SSIM and\\nLPIPS metrics in this multi-view evaluation protocol on high-resolution images.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='10 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\n6 Conclusions and Limitations\\nWe have presented SMPLitex, a generative model for 3D human appearance that enables the\\nestimation of 3D human textures from single images. SMPLitex leverages recent image dif-\\nfusion models for 2D image synthesis and uses pixel-to-surface correspondence estimation\\nto bridge the gap between 2D images and 3D surfaces. By conditioning the diffusion model\\nto the visible pixels of a human in a single view, SMPLitex is able to synthesize a complete\\ntexture map of the subject, outperforming current methods based on GANs or V AEs.\\nDespite the convincing quality of the results, SMPLitex suffers from limitations as well.\\nIf the subject on the input image is significantly occluded or if surface-to-pixel correspon-\\ndence fails, SMPLitex sampling is weakly conditioned hence it can potentially generate tex-\\ntures that do not match well the subject. Similarly, when sampling the model with text,\\nif prompts are too general or not related to humans, output textures can exhibit unrealistic\\nfacial or body features such as deformed faces or missing limbs.\\nAcknowledgments. This work has been partially funded by: the European Union’s Hori-\\nzon 2020 research and innovation program under grant agreement No 899739 (H2020-\\nFETOPEN-2018-2020 CrowdDNA project); and by the Universidad Rey Juan Carlos through\\nthe Distinguished Researcher position INVESDIST-04 under the call from 17/12/2020.\\nReferences\\n[1] Badour AlBahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, and Jia-Bin\\nHuang. Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Condi-\\ntional StyleGAN. ACM Transactions on Graphics (TOG), 40(6):1–11, 2021.\\n[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-\\nMoll. Video Based Reconstruction of 3D People Models. In Proc. of Computer Vision\\nand Pattern Recognition (CVPR), pages 8387–8397, 2018.\\n[3] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic monocular\\n3d reconstruction of humans wearing clothing. InProc. of Computer Vision and Pattern\\nRecognition (CVPR), pages 1496–1505, 2022. doi: 10.1109/CVPR52688.2022.00156.\\n[4] Miguel Ángel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander T\\nToshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht,\\nAfshin Dehghan, and Joshua M. Susskind. GAUDI: A Neural Architect for Immer-\\nsive 3D Scene Generation. In Advances in Neural Information Processing Systems\\n(NeurIPS), 2022.\\n[5] Sihun Cha, Kwanggyoon Seo, Amirsaman Ashtari, and Junyong Noh. Generating\\nTexture for 3D Human Avatar from a Single Image using Sampling and Refinement\\nNetworks. In Computer Graphics Forum, volume 42, pages 385–396, 2023.\\n[6] Xiangguang Chen, Ye Zhu, Yu Li, Bingtao Fu, Lei Sun, Ying Shan, and Shan Liu. Ro-\\nbust Human Matting via Semantic Guidance. In Proceedings of the Asian Conference\\non Computer Vision (ACCV), 2022.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 11\\n[7] Soon Yau Cheong, Armin Mustafa, and Andrew Gilbert. KPE: Keypoint Pose Encod-\\ning for Transformer-based Image Generation. In British Machine Vision Conference\\n(BMVC), 2022.\\n[8] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Improving Diffusion Models\\nfor Inverse Problems using Manifold Constraints. In Advances in Neural Information\\nProcessing Systems (NeurIPS), 2022.\\n[9] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy,\\nWayne Wu, and Ziwei Liu. StyleGAN-Human: A Data-Centric Odyssey of Human\\nGeneration. In Proc. of European Conference on Computer Vision (ECCV), 2022.\\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks.\\nCommunications of the ACM, 63(11):139–144, 2020.\\n[11] Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, and Victor Lempitsky.\\nCoordinate-based texture inpainting for pose-guided human image generation. InProc.\\nof Computer Vision and Pattern Recognition (CVPR), 2019.\\n[12] Artur Grigorev, Karim Iskakov, Anastasia Ianina, Renat Bashirov, Ilya Zakharkin,\\nAlexander Vakhitov, and Victor Lempitsky. StylePeople: A Generative Model of Full-\\nbody Human Avatars. In Proc. of Computer Vision and Pattern Recognition (CVPR),\\n2021.\\n[13] Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human\\npose estimation in the wild. In Proc. of Computer Vision and Pattern Recognition\\n(CVPR), pages 7297–7306, 2018.\\n[14] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll,\\nand Christian Theobalt. Real-time Deep Dynamic Characters. ACM Transactions on\\nGraphics, 40(4), aug 2021.\\n[15] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. Arch++:\\nAnimation-ready clothed human reconstruction revisited. In Proc. of IEEE Interna-\\ntional Conference on Computer Vision (ICCV), pages 11026–11036, 2021.\\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.\\nAdvances in Neural Information Processing Systems (NeurIPS), 33:6840–6851, 2020.\\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Grit-\\nsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim\\nSalimans. Imagen Video: High Definition Video Generation with Diffusion Models.\\narXiv:2210.02303, 2022.\\n[18] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi,\\nand David J. Fleet. Video diffusion models. In Advances in Neural Information Pro-\\ncessing Systems (NeurIPS), 2022.\\n[19] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei\\nLiu. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars.\\nACM Transactions on Graphics (Proc. SIGGRAPH), 41(4), 2022.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='12 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language\\nModels. In International Conference on Learning Representations (ICLR), 2022.\\n[21] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-\\nshot text-guided object generation with dream fields. In Proc. of Computer Vision and\\nPattern Recognition (CVPR), 2022.\\n[22] Wang Jian, Zhong Yunshan, Li Yachun, Zhang Chi, and Wei Yichen. Re-Identification\\nSupervised Texture Generation. Proc. of Computer Vision and Pattern Recognition\\n(CVPR), 2019.\\n[23] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neu-\\nMan: Neural Human Radiance Field from a Single Video. In Proceedings of the Euro-\\npean conference on computer vision (ECCV), 2022.\\n[24] Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, and Ziwei Liu.\\nText2Human: Text-Driven Controllable Human Image Generation. ACM Transactions\\non Graphics (Proc. SIGGRAPH), 41(4), 2022.\\n[25] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Learning\\ncategory-specific mesh reconstruction from image collections. In Proc. of Computer\\nVision and Pattern Recognition (CVPR), pages 371–386, 2018.\\n[26] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo\\nAila. Analyzing and Improving the Image Quality of StyleGAN. In Proc. of Computer\\nVision and Pattern Recognition (CVPR), 2020.\\n[27] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. Clip-\\nmesh: Generating textured meshes from text using pretrained image-text models. SIG-\\nGRAPH Asia 2022 Conference Papers, December 2022.\\n[28] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai\\nFieraru, and Cristian Sminchisescu. DreamHuman: Animatable 3D Avatars from Text.\\narXiv preprint arXiv:2306.09329, 2023.\\n[29] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll. 360-degree textures of peo-\\nple in clothing from a single image. In International Conference on 3D Vision (3DV),\\npages 643–653, 2019.\\n[30] Kathleen M Lewis, Srivatsan Varadharajan, and Ira Kemelmacher-Shlizerman. TryOn-\\nGAN: Body-Aware Try-On via Layered Interpolation. ACM Transactions on Graphics\\n(Proc. SIGGRAPH), 40(4), 2021.\\n[31] Zhe Li, Zerong Zheng, Hongwen Zhang, Chaonan Ji, and Yebin Liu. AvatarCap: Ani-\\nmatable Avatar Conditioned Monocular Human V olumetric Capture. InProc. of Euro-\\npean Conference on Computer Vision (ECCV), 2022.\\n[32] Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollhöfer, Florian Bernard,\\nHyeongwoo Kim, Wenping Wang, and Christian Theobalt. Learning Dynamic Tex-\\ntures for Neural Rendering of Human Actors. IEEE Transactions on Visualization and\\nComputer Graphics, 27(10):4009–4022, 2021. doi: 10.1109/TVCG.2020.2996594.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 13\\n[33] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. DeepFashion: Pow-\\nering Robust Clothes Recognition and Retrieval with Rich Annotations. In Proc. of\\nComputer Vision and Pattern Recognition (CVPR), 2016.\\n[34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J\\nBlack. SMPL: A Skinned Multi-Person Linear Mdel. ACM Transactions on Graphics\\n(TOG), 34(6):1–16, 2015.\\n[35] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and\\nLuc Van Gool. RePaint: Inpainting using Denoising Diffusion Probabilistic Models. In\\nProc. of Computer Vision and Pattern Recognition (CVPR), pages 11461–11471, 2022.\\n[36] Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, and Zhouhui Lian. Control-\\nlable Person Image Synthesis with Attribute-Decomposed GAN. In Proc. of Computer\\nVision and Pattern Recognition (CVPR), 2020.\\n[37] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka.\\nText2mesh: Text-driven neural stylization for meshes. In Proc. of Computer Vision\\nand Pattern Recognition (CVPR), 2022.\\n[38] Aymen Mir, Thiemo Alldieck, and Gerard Pons-Moll. Learning to transfer texture from\\nclothing images to 3d humans. In Proc. of Computer Vision and Pattern Recognition\\n(CVPR), pages 7023–7034, 2020.\\n[39] Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li,\\nand Shigeo Morishima. Siclope: Silhouette-based clothed people. InProc. of Computer\\nVision and Pattern Recognition (CVPR), 2019.\\n[40] Natalia Neverova, Riza Alp Guler, and Iasonas Kokkinos. Dense Pose Transfer. In\\nProc. of European Conference on Computer Vision (ECCV), 2018.\\n[41] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural Articulated Ra-\\ndiance Field. In Proc. of IEEE International Conference on Computer Vision (ICCV),\\npages 5762–5772, 2021.\\n[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei\\nZhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic\\nHuman Bodies. InProc. of IEEE International Conference on Computer Vision (ICCV),\\n2021.\\n[43] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-\\nto-3D using 2D Diffusion. In International Conference on Learning Representations\\n(ICLR), 2023.\\n[44] Sergey Prokudin, Michael J Black, and Javier Romero. SMPLpix: Neural Avatars from\\n3D Human Models. In Proc. of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision, pages 1810–1819, 2021.\\n[45] Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, and Francesc Moreno-Noguer. Un-\\nsupervised Person Image Synthesis in Arbitrary Poses. InProc. of Computer Vision and\\nPattern Recognition (CVPR), pages 8620–8628, 2018.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='14 DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX\\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervision. In International confer-\\nence on machine learning, pages 8748–8763. PMLR, 2021.\\n[47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\\nHierarchical text-conditional image generation with clip latents. arXiv preprint\\narXiv:2204.06125, 2022.\\n[48] Robin Rombach and Patrick Esser. Stable diffusion v1-4 model card. https:\\n//huggingface.co/CompVis/stable-diffusion-v1-4 , 2023. Online;\\naccessed 1-September-2023.\\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Om-\\nmer. High-Resolution Image Synthesis with Latent Diffusion Models. In Proc. of\\nComputer Vision and Pattern Recognition (CVPR), pages 10684–10695, 2022.\\n[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\\nAberman. DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-\\nDriven Generation. In cvpr, 2023.\\n[51] Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and Hao Li. Photorealistic\\nfacial texture inference using deep neural networks. In Proc. of Computer Vision and\\nPattern Recognition (CVPR), 2017.\\n[52] Kripasindhu Sarkar, Vladislav Golyanik, Lingjie Liu, and Christian Theobalt. Style\\nand Pose Control for Image Synthesis of Humans from a Single Monocular View.\\narXiv:2102.11263, 2021.\\n[53] Kripasindhu Sarkar, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. Human-\\nGAN: A Generative Model of Humans Images. In International Conference on 3D\\nVision (3DV), pages 258–267. IEEE, 2021.\\n[54] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Ar-\\nsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and\\nVictor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions.\\nIn Proc. of IEEE International Conference on Computer Vision (ICCV) , pages 2149–\\n2159, 2022.\\n[55] Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan\\nLaptev, and Cordelia Schmid. Learning from synthetic humans. In Proc. of Computer\\nVision and Pattern Recognition (CVPR), 2017.\\n[56] Vikram V oleti, Alexia Jolicoeur-Martineau, and Christopher Pal. MCVD: Masked Con-\\nditional Video Diffusion for Prediction, Generation, and Interpolation. In Advances in\\nNeural Information Processing Systems (NeurIPS), 2022.\\n[57] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea\\nTagliasacchi, and Mohammad Norouzi. Novel View Synthesis with Diffusion Mod-\\nels. In International Conference on Learning Representations (ICLR), 2023.'), Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-14T07:34:17+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-14T07:34:17+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/0272.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='DAN CASAS AND MARC COMINO-TRINIDAD: SMPLITEX 15\\n[58] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira\\nKemelmacher-Shlizerman. HumanNeRF: Free-viewpoint Rendering of Moving People\\nfrom Monocular Video. In Proc. of Computer Vision and Pattern Recognition (CVPR),\\npages 16210–16220, 2022.\\n[59] Xiangyu Xu and Chen Change Loy. 3D human texture estimation from a single image\\nwith transformers. In Proc. of IEEE International Conference on Computer Vision\\n(ICCV), 2021.\\n[60] Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A Jeni, and Fernando De la\\nTorre. 3D Human Pose, Shape and Texture from Low-Resolution Images and Videos.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 44(9):\\n4490–4504, 2021.\\n[61] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu.\\nFunction4d: Real-time human volumetric capture from very sparse consumer rgbd sen-\\nsors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021),\\nJune 2021.\\n[62] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long,\\nFeida Zhu, Kang Du, and Min Zheng. AvatarVerse: High-quality & Stable 3D Avatar\\nCreation from Text and Pose. arXiv preprint arXiv:2308.03610, 2023.\\n[63] Fang Zhao, Shengcai Liao, Kaihao Zhang, and Ling Shao. Human Parsing Based\\nTexture Transfer from Single Image to 3D Human via Cross-View Consistency. In\\nAdvances in Neural Information Processing Systems (NeurIPS), 2020.\\n[64] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scal-\\nable person re-identification: A benchmark. In Proc. of IEEE International Conference\\non Computer Vision (ICCV), pages 1116–1124, 2015.\\n[65] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. PaMIR: Parametric Model-\\nConditioned Implicit Representation for Image-based Human Reconstruction. IEEE\\nTransactions on Pattern Analysis and Machine Intelegence, 2021.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1'}, page_content='Muscles in Time: Learning to Understand Human\\nMotion by Simulating Muscle Activations\\nDavid Schneider∗† Simon Reiß† Marco Kugler† Alexander Jaus† Kunyu Peng†\\nSusanne Sutschet† M. Saquib Sarfraz‡ Sven Matthiesen† Rainer Stiefelhagen†\\nAbstract\\nExploring the intricate dynamics between muscular and skeletal structures is pivotal\\nfor understanding human motion. This domain presents substantial challenges,\\nprimarily attributed to the intensive resources required for acquiring ground truth\\nmuscle activation data, resulting in a scarcity of datasets. In this work, we address\\nthis issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle\\nactivation dataset. For the creation of MinT, we enriched existing motion capture\\ndatasets by incorporating muscle activation simulations derived from biomechan-\\nical human body models using the OpenSim platform, a common approach in\\nbiomechanics and human motion research. Starting from simple pose sequences,\\nour pipeline enables us to extract detailed information about the timing of muscle\\nactivations within the human musculoskeletal system. Muscles in Timecontains\\nover nine hours of simulation data covering 227 subjects and 402 simulated muscle\\nstrands. We demonstrate the utility of this dataset by presenting results on neural\\nnetwork-based muscle activation estimation from human pose sequences with two\\ndifferent sequence-to-sequence architectures.\\nData and code are provided under https://simplexsigil.github.io/mint.\\n1 Introduction\\nLike prisoners in Plato’s cave, neural networks for human motion understanding often rely on indirect\\nrepresentations rather than direct, biologically grounded data. In Plato’s allegory, prisoners in a\\ncave see only shadows cast on the wall, not the true objects. Similarly, neural networks trained on\\naccessible data, such as RGB and depth-based video recordings or motion capture, only perceive\\nsurface-level appearance of motion in contrast to the inner mechanics of the human body.\\nThis reliance on external visual observations provides an incomplete understanding of the true\\ncomplexities of human motion. Just as the prisoners lack a direct view of the objects casting the\\nshadows, current models lack exposure to the internal workings of the human body, such as the\\nmuscle activations driving motion. This gap limits their ability to develop an in-depth understanding\\nof physical exertion, motion difficulty, and mass impact on the body.\\nOur community has progressed from capturing human motion with camera sensors and predicting\\nactivities to pose-based recognition systems that account for the body and its motion over time. These\\nadvances, while significant, still overlook the interplay of muscle activations, which are the root of\\npose sequences and patterns.\\n∗Corresponding author: david.schneider@kit.edu\\n†Karlsruhe Institute of Technology\\n‡Mercedes-Benz Tech Innovation\\nPreprint.\\narXiv:2411.00128v1  [cs.CV]  31 Oct 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2'}, page_content='Video SMPL Biomechanical models (OpenSim)\\nMuscles in Time Dataset\\n227 subjects\\n 402 muscle strands\\n 9.8h muscle activations\\nGastrocnemius medialis Tibialis anterior Biceps femoris l.h.\\n…\\nFigure 1: Simulation pipeline of the Muscles in Time dataset. The SMPL representation is extracted\\nfrom videos, then, the SMPL represented motions are mapped to bio-mechanically validated human\\nbody models to simulate fine-grained muscle activation, connecting computer vision with biomechan-\\nical research. Bottom right: two activation sequences for exemplary muscles. Images from [47, 15]\\n.\\nCollecting electromyographic (EMG) data or more commonly used surface electromyographic\\n(sEMG) data, as a measure of muscle activation, presents challenges. It is resource intensive,\\nrequiring specialized equipment, controlled environments, and is an invasive procedure. Existing\\nEMG and sEMG datasets are small, limited in scope, and not representative of the variety of human\\nmotions. These limitations hinder the development of neural networks that can generalize across\\ndifferent types of motion and subjects.\\nWhile acknowledging the contributions of EMG and sEMG datasets, we identify an opportunity to\\nsupplement this domain with a synthetic dataset that overcomes some limitations of real-world data\\ncollection. The strength of our dataset lies in its scale and detail of muscle activation data, a feat not\\nachievable through conventional methods alone.\\nEvery dataset, simulated or real, has domain-specific fidelity and relevance. Real-world recordings\\noffer authenticity that underpins our understanding of human biomechanics with nuances, such as\\nEMG measurements being subject-specific and varying over the course of one day. Simulated datasets,\\nlike ours, offer a complementary perspective by providing comprehensive data for the understanding\\nof muscle activation patterns through a scalable data acquisition pipeline.\\nIn this work, we present a comprehensive large-scale dataset incorporating muscle activation in-\\nformation. We enrich existing motion capture datasets with muscle activation simulations from\\nbiomechanical models of the human body. Our pipeline uses simple pose and shape sequences\\nwith estimated weight and mass of the human body to simulate muscle activations for individual\\nmovements. Using this, we generate the muscles’ activation that fit the provided human motions.\\nFigure 1 provides an overview of our pipeline.\\nWe showcase the utility of muscle activations as an additional data type for human motion under-\\nstanding and gather insights by visualizing the intricate details of our data. Our dataset, the first of its\\nmagnitude and detail, describes muscle activation across a wide array of movements. By enhancing\\nthe current set of tools available to researchers, we expand the potential for scientific investigation\\nand innovation in the study of human motion.\\n2 Related Work\\nHuman Motion Analysis and DatasetsEMG-based muscle activation analysis is a well-established\\nfield in biomechanical research. Still, publicly available databases including experimentally measured\\nmuscle excitation using sEMG are often small in size or cover a small range of muscles or motion\\nvariations [21, 30, 79, 25, 51, 46, 61, 42, 34, 50, 65]. The dataset proposed by Zhang et al. [79]\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3'}, page_content='Table 1: Comparison between recent muscle activation datasets and Muscles in Time.\\nYear SubjectsAct. Vals. DurationActionsGRF RGB DepthActivationSkeletonDescription\\nCamargo et al. [5] 2021 22 11 10 min 4 × × × ✓ × ×\\nFeldotto et al.[21] 2022 5 7 10 min 4 × × × ✓ × ×\\nKIMHu [30] 2023 20 4 10 h 3 ✓ ✓ ✓ ✓ ✓ ×\\nMuscleMap [55] 2023 N/A 20a ∼25 hb 135 ✓ ✓ ✓ × ✓ ×\\nMiA [10] 2023 10 8 12.5 h 15 ✓ ✓ × × ✓ ×\\nMinT (ours) 2024 227 402c,d 10 h 187 ✓d ✓d,e ✓d ✓d ✓ ✓\\na Clip-wise binary labels. b Coarse estimation based on 15,004 clips of 3-9s. c Muscle strands, some\\nmuscles represented by multiple strands. d Simulated data. e From [64]\\ncontains 5 persons and leveraged 8 EMG sensors. The KIMHu dataset [30], for example, includes\\nsEMG data of four upper limb muscles measured during different arm exercises performed by 20\\nsubjects. The MIA Dataset [10] includes sEMG signals for eight muscles in total (upper and lower\\nlimb) across 10 subjects who performed 15 different exercises, e.g., running, jumping jacks, squats,\\nand elbow punches. MuscleMap is a video-based muscle activation estimation dataset, which assigns\\nbinary muscle activation labels to action categories, involving20 muscle groups and 135 actions [55].\\nIn Table 1, we provide a comparison of multiple recent EMG datasets to MinT. Most notably MinT\\nfeatures a significantly larger number of subjects, a larger number of activation measurements and a\\ndiverse range of motions.\\nOpenSim is an open-source software platform for musculoskeletal modeling, simulation, and anal-\\nysis. It is widely used in various research areas such as biomechanics research, orthopedics and\\nrehabilitation science, and medical device design [16, 66]. The state-of-the-art process in OpenSim\\nfor simulating muscle activations of a certain task requires subject-specific motion and force data. In\\nmost cases, these data are obtained through experimental studies, which can be time-consuming and\\nresource-intensive.\\nIn a related field, musculoskeletal humanoid control and simulation focuses on developing computa-\\ntional models and control strategies for simulating human motion with musculoskeletal detail. Recent\\nwork by Jiang et al. [35], Caggiano et al. [4], Feng et al. [23], and He et al. [83, 29] has advanced\\nmethods for efficient and realistic simulation of muscle-actuated characters. While these approaches\\ndiffer from OpenSim’s focus, they highlight the broader interest in understanding and simulating\\nhuman musculoskeletal dynamics.\\nSkeleton-based Vision ModelsSkeleton-based action recognition [ 22, 1] is pivotal in decoding\\nhuman actions from video footage, providing a streamlined and insightful depiction of human poses\\nand movements that remains invariant to changes in appearance, illumination, and backdrop. This\\napproach enhances the identification of dynamic skeletal characteristics essential for precise action\\nrecognition, finding utility across surveillance, human-computer interaction, and medical fields.\\nThe goal of skeleton-based action recognition is to classify actions based on skeletal geometry\\ninformation [36, 44, 49, 19, 56, 74, 54, 72, 7]. Predominantly, the techniques employed are based\\non graph convolutional neural networks (GCN)[38, 76, 68, 9, 77, 8], with newer methods adopting\\ntransformer architectures [69, 58, 41, 81, 17, 73]. Chen et al.[8] proposed channel-wise topology\\nrefinement graph convolution for skeleton-based action recognition. Yanet al.[75] proposed skeleton\\nmasked auto encoder to achieve skeleton sequence pretraining which delivers promising benefits for\\nthe skeleton based action recognition. Apart from the GCN and transformer based models, PoseC3D\\nis proposed by Duan et al.[19] to use 3D convolutional neural networks on the heat map figures\\npainted by the skeleton joints.\\nSequence-to-sequence ModelsSequence to sequence models [ 52, 37, 11, 80, 43, 67, 24] are a\\nclass of deep neural network architectures designed to transform sequences from one domain into\\nsequences in another domain, typically used in applications such as machine translation, speech\\nrecognition, and text summarization. These models generally consist of an encoder that processes the\\ninput sequence and a decoder that generates the output sequence, facilitating the learning of complex\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4'}, page_content='Figure 2: The AMASS body model with specific indices mapped onto the OpenSim lower body\\nmodel by Lai et al. [40] (middle) and model of the thoracolumbar region by Bruno et al. [3] (right).\\nBest viewed by zooming in.\\nsequence mappings through recurrent neural networks (RNNs) [ 45, 53, 57, 33] or transformer-\\nbased architectures [ 18, 32]. Chan et al. [6] proposed Imputer method by using imputation and\\ndynamic programming to achieve sequence modelling. Colombo et al. [13] used guiding attention\\nfor sequence-to-sequence modelling for dialogue activities prediction. Rae et al. [60] proposed\\ncompressed transformer architecture for long-range sequence modelling. Foo et al. [24] proposed a\\nunified pose sequence modelling method for human behavior understanding.\\n3 The Muscles In Time dataset\\nTo develop the Muscles in Time (MinT)framework, we harnessed the comprehensive AMASS\\ndataset, which consolidates various marker-based motion capture (mocap) sequences into a uniform\\nrepresentation using the MoSh++ method, resulting in Skinned Multi-Person Linear Model (SMPL)\\nparametric representations for body pose and shape. AMASS amalgamates mocap data from multiple\\nsources, including the KIT Whole-Body Human Motion Database [48], BMLrub, and BMLmovi [27],\\nencompassing over 11,000 motion captures from more than 300 subjects. This extensive collection\\nenables the analysis of a broad array of human movements, providing a rich basis for studying diverse\\nmotion patterns.\\nThe SMPL model serves as a pivotal link, translating mocap data from AMASS into mesh rep-\\nresentations which we use to transfer the data into a format compatible with the OpenSim [ 15]\\nplatform. OpenSim is instrumental in constructing intricate biomechanical models that simulate the\\nmusculoskeletal system’s physical and mechanical properties, allowing for an in-depth analysis of\\nhuman motion. These models are intricate, requiring precise definitions of joints, masses, inertia, and\\nmuscle parameters, such as maximum isometric force, which act as the force-generating actuators.\\nIn this work, we abstain from developing new biomechanical models due to the complexity and\\nexpertise required. Instead, we utilize established, pre-validated models, specifically the lower body\\nmodel by Lai et al. [ 40] and the thoracolumbar region body model by Bruno et al. [3], see Figure 2.\\nThese models simulate muscle activations for an extensive network of individual muscle strands\\nacross various muscle groups, providing a comprehensive simulation of human musculature. A\\ndetailed list of these muscle groups and their function in the human body is provided in the Appendix.\\nTailoring body model parameters to an individual’s anatomical properties results in similar difficulties\\nas with the creation of new body models, therefore parameters are commonly used as specified in the\\nvalidated original models [40, 3], in the OpenSim community. We follow this approach, providing\\nsimulation results for standard models rather than subject-specific human bodies.\\nTo integrate human motion data from AMASS with OpenSim, we map virtual mocap markers to the\\nSMPL-H body mesh’s surface vertices, following the method proposed by Bittner et al. [2]. This\\nresults in a selection of 67 strategically placed vertices that represent marker positions on the body\\nmesh, visualized on the left of Figure 2. We deliberately exclude soft tissue dynamics from the\\nSMPL-H mesh generation to maintain consistent marker positions during motion.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5'}, page_content='Figure 3: Approximated weight and height distribution of the analysed subjects in the MinT dataset.\\nDespite OpenSim’s automatic scaling capabilities, manual adjustments of marker positions are some-\\ntimes necessary to reconcile differences between simulated and real-world data. These adjustments\\nare made on a subject-specific basis, rendering our pipeline semi-automatic. The manually adjusted\\nmarker positions are documented and shared to ensure the reproducibility of our simulations.\\nAMASS lacks data on external ground reaction forces or contact forces, which are crucial for realistic\\nmotion simulation. To address this, we integrate the OpenSimAD [20] implementation used in the\\nOpenCap [70] project, which calculates ground reaction forces based on kinematic data and the\\nmusculoskeletal model. We employ a tailored parameter setup to optimize the trajectory problem,\\nbalancing computational load and accuracy.\\nKinematic data is analyzed using OpenSim’s Inverse Kinematicsmethod. Muscle activations for\\nthe lower body are derived from a trajectory optimization problem described in [70]. The estimated\\nground reaction forces from this problem serve as inputs for the Static Optimizationmethod, which\\ncalculates muscle activations for the thoracolumbar region.\\nDue to the computational demands of the trajectory optimization problem, we process the data\\nin segments, ensuring manageable computation times without compromising the continuity of the\\nmotion capture sequences. We implement overlapping buffers to mitigate inaccuracies during segment\\nprocessing, discarding data that fails to meet our stringent error tolerance criteria to maintain a high\\nstandard of data quality. Further details on implementation and design decisions of our simulation\\nprocess are presented in the Appendix.\\nThe Muscles in Time (MinT) dataset represents a significant contribution to the field of biomechanical\\nand computer vision simulation. By integrating and refining existing methodologies, we present a\\nrobust pipeline that facilitates the accurate simulation of human muscles in motion by combining\\nestablished biomechanical models with high quality mocap data. To ensure reproducibility, we will\\nrelease all relevant data and details of our simulation process to the scientific community.\\n3.1 Dataset Composition\\nDue to missing information on external forces based on object interactions, inaccurate motion capture\\nrecordings or non-converging simulations, the MinT dataset covers a subset of its originating datasets\\nin AMASS and does not follow their respective dataset statistics.\\nAnthropometrics While the motion capture recordings in AMASS provide gender labels, information\\nabout subjects height and weight is approximated from the SMPL body model. Body weight is\\ncalculated by volume resulting from average shape parameters, which follows the approach of\\nBittner et al. [2]. The weight is relevant for the calculation of ground reaction forces and the\\ndistribution of weight in the model, affecting the muscle activation in different parts of the body.\\nThe Figure 3 shows the distribution of weight, indicating significant diversity. Underweight subjects\\nare slightly underrepresented in the dataset, subjects in the obese range are well represented.\\nComposition of SubdatasetsWithin AMASS,MinT is limited to the subdatasets EyesJapan, BMLrub,\\nKIT, BMLmovi, and TotalCapture. Figure 9 in the appendix shows the ratio of the originating\\nsubdatasets in our final simulation results as well as the average sequence length within these\\nsubdatasets. The short sequences in BMLmovi typically depict single activities, while the longer\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6'}, page_content='Type of \\nMovement\\nStatic \\nActions\\nDynamic \\nActions\\nBody \\nInteraction\\nBody Part \\nMovement Activity\\nFigure 4: Prevalence of different motions in the MinT dataset.\\nones for example in JapanEyes capture a more diverse range of motions within a single sequence.\\nSince we compute activation information for shorter segments and rejoin them afterwards, longer\\nsequences are more prone to gaps in the analysis due to individually failing segment computations.\\nMotion DiversityFigure 4 displays the frequencies of grouped activities on a logarithmic scale.\\nThe action labels are based on the BABEL dataset, a large annotation dataset which is coupled with\\nAMASS. Most interesting are dynamic actions, since expected muscle activations for simple dynamic\\nactions are well documented and we present a short qualitative analysis based on such actions in\\nSection 3.2.\\n3.2 Data Analysis, Validation, and Visualization\\nIn Figure 5 (left) we explore the interrelation between different activities by investigating our\\nsimulated muscle activation time-series. To this end, we extract features from the temporal muscle\\nactivation sequences using tsfresh [ 12], a commonly used framework in time series analysis that\\nextracts a feature vector based on time series characteristics such as mean, skewness, standard\\ndeviation etc. We chose distinct and descriptive groups of activities from the BMLmovi subset such\\nas jumping, kicking, stepping and walking, the resulting features were normalized and clustered\\nusing FINCH [62] and visualized with h-NNE [63]. It can be observed, that activities do not only\\ncluster together based on variations within the same category (e.g., different types of jumps, including\\njumping jacks), but also align closely across different categories, when they share similar motion\\npatterns (e.g. sideways movements). This underlines the descriptive information contained in our\\nsimulated muscle activation sequences for characterizing activities.\\n4 Motion to Muscle Activation Estimation Benchmark\\nWhile OpenSim provides a means for simulating muscle activations, it is both highly compute\\nintensive as well as sensitive regarding hyper parameters as described in Section 6. These properties\\nlimit it to be used by experts in an offline manner and prevent usage in everyday applications. In this\\nsection we explore the usage of MINT as a training dataset for the estimation of muscle activation\\nbased on pose motion. Such networks provide muscle activation estimation in an instant and can\\neasily be deployed for various downstream tasks.\\nGiven pose motion sequences, we use the preprocessing step defined by [28] which adjusts skeletal\\nstructure to a uniform format and normalizes positions and enriches the resulting data points with\\nadditional features. This procedure maps each input to a 263-dim descriptor, resulting in samples\\nof the form x = [x1, ..., xT ], xt ∈ R263. For training our models we segment the resulting data\\ninto clips of 1.4 second sampled at 20 frames per second, resulting in T = 28input frames. Given\\na network fΘ : RT×d 7→ RT×m we predict f(x) = y with y = [y1, ..., yT ], m = 402being the\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7'}, page_content='Figure 5: Left: Clustering of multiple activities within the BMLmovi dataset by muscle activation\\nfeatures. Right: Column-wise color coded histograms of areas under muscle activation curves for\\n402 muscle strains, sorted by histogram medians. Log-normalized color map, best displayed in color.\\nnumber of individual muscle strain activations simulated in our dataset, consisting of 80 lower\\nbody muscle strains from [ 20] and 322 muscle strains for the upper thoracolumbar region body\\nmodel [3]. Evaluation is performed by calculating Root Mean Squared Error (RMSE), Pearson\\nCorrelation Coefficient (PCC), and Symmetric Mean Absolute Percentage Error (SMAPE). RMSE is\\ncommonly used but highly susceptible to data scaling, resulting in significantly lower error values\\nfor downscaled data. In practice, EMG signals vary strongly between subjects, scaling of signals\\nis therefore a common preprocessing step. PCC is a good indicator for muscle activation series\\nsimilarity, since it is scale and offset invariant. SMAPE allows for considering fixed offsets as error\\nwhile being less sensitive to scaling in comparison to RMSE. PCC and SMAPE are calculated for\\neach muscle strain individually and averaged. For our benchmark we use the train, val and test splits\\ndefined by the BABEL dataset [59]. Evaluation results are reported separately for muscles of the\\nupper and lower body model.\\n5 Experiments\\nWe evaluate five different architectures onMINT. Since we make use of human motion as input for\\nour predictor, we adapted a common architecture for motion-to-motion prediction from [78] to the\\ntask of motion-to-muscle activation prediction by simply exchanging its prediction head. We further\\nevaluate a Long Short-Term Memory (LSTM) [31], a fully convolutional network (FConv) [26], a\\nMamba2 Mixer model [14] and a simple transformer architecture [71] with 16 transformer layers,\\nresults for the lower and upper body model are listed in Table 2. All models are trained from scratch\\nfor 300k iterations with a batch size of 256 unless noted otherwise. More details on the model\\nimplementations can be found in the supplementary.\\nThe evaluated transformer architecture showed the best results as compared to the adapted VQ-V AE\\nmodel, LSTM, FConv and Mamba in all metrics on all evaluated motion types. The results of the\\nexperiment also show the importance of reporting PCC and SMAPE, since the differences on RMSE\\nare marginal while PCC shows significant improvements as does SMAPE. We suspect this to be\\nthe case, since many muscles in the human body are mostly relatively inactive unless required for\\nspecific motions. For a simple analysis of this effect, we calculated the integral for each individual\\nground truth muscle activation sequence in all our validation set chunks and created 402 color coded\\nhistograms that are sorted by median and vertically displayed side by side on the right hand side of\\nFigure 5 (one column in the image is a single muscle activation integral area frequency histogram). A\\nwide range of muscles are rarely activated, resulting in the majority of activation sequences displaying\\nintegral areas significantly below 0.1 or 0.05. This property is challenging for RMSE and SMAPE,\\naverage RMSE reports a small error, since most activations are close to zero and SMAPE reports a\\nhigh percentage error, since a deviation from a close to zero value is more likely to result in a high\\npercentage deviation. For similar reasons, the upper body model displays lower RMSE and higher\\nSMAPE, the upper body model contains a larger number of small and rarely activated muscles in\\ncontrast to the lower body model.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8'}, page_content='Table 2: Human motion-to-muscle activation prediction results for the lower- and upper body model.\\nAct VQ-V AE [78] FConv [26] LSTM [31] Mamba2 [14] Transformer [71]\\nR↓ S↓ P↑ R↓ S↓ P↑ R↓ S↓ P↑ R↓ S↓ P↑ R↓ S↓ P↑\\nLower body model\\nall 0.058 59.7 0.40 0.052 66.0 0.49 0.052 57.8 0.48 0.051 55.4 0.49 0.048 45.1 0.54\\njump 0.062 66.7 0.52 0.053 68.1 0.66 0.052 62.2 0.67 0.051 60.8 0.68 0.051 52.3 0.71\\nkick 0.069 69.1 0.38 0.057 74.9 0.55 0.058 66.5 0.55 0.059 67.6 0.55 0.053 54.8 0.62\\nstand 0.056 60.0 0.42 0.049 64.4 0.51 0.050 58.2 0.51 0.049 55.1 0.52 0.046 45.0 0.58\\nwalk 0.053 57.7 0.66 0.046 61.7 0.73 0.045 53.7 0.73 0.045 50.4 0.74 0.044 42.4 0.77\\njog 0.059 64.8 0.58 0.052 69.1 0.66 0.050 61.5 0.68 0.047 58.2 0.69 0.046 51.1 0.71\\ndance 0.070 71.4 0.40 0.064 76.0 0.59 0.063 71.5 0.57 0.063 70.2 0.57 0.057 58.5 0.65\\nUpper body model\\nall 0.041 115.3 0.32 0.034 114.8 0.47 0.035 111.1 0.48 0.034 112.2 0.50 0.033 107.7 0.55\\njump 0.064 118.1 0.38 0.052 119.6 0.54 0.054 115.4 0.56 0.053 117.2 0.58 0.052 112.7 0.63\\nkick 0.058 122.2 0.35 0.048 121.5 0.55 0.048 118.1 0.57 0.048 119.4 0.58 0.044 114.8 0.65\\nstand 0.039 117.6 0.34 0.031 118.2 0.48 0.031 114.2 0.49 0.030 114.9 0.51 0.028 110.5 0.55\\nwalk 0.028 110.2 0.43 0.021 109.8 0.55 0.022 105.6 0.57 0.020 106.8 0.59 0.019 102.6 0.63\\njog 0.040 117.1 0.52 0.034 118.5 0.64 0.032 113.9 0.66 0.031 115.4 0.66 0.029 110.8 0.71\\ndance 0.046 127.3 0.29 0.041 129.5 0.48 0.044 126.7 0.48 0.039 128.2 0.49 0.036 121.8 0.59\\nR: RSME S: SMAPE P: PCC\\nTo provide a more detailed analysis we list the results on the collection of all available muscle strains\\nin the main paper, but list further evaluations on carefully chosen subsets of major motion inducing\\nbody muscles in the appendix. We recommend future users of our dataset to consider actively\\nevaluating on either the full range of provided muscle activations or choosing one of these muscle\\nstrand subsets depending on their specific application. Please also see the appendix for additional\\nexperiments as well as a comparison to the work of [10].\\n5.1 Qualitative Results\\nFigure 6: Example lower body muscle activations (split in left and right muscle strands) for the\\nactions kick and jumping jacks. It is clearly visible that the kick is performed with the left leg. During\\njumping jacks, gluteus mediusand rectus femorisare activated alternatingly for both legs.\\nIn Figure 6 we list two examples from our dataset, one displaying the actionkick, the other displaying\\nthe action jumping jacks, predictions are calculated with the 8-layer transformer architecture. The\\nfigure displays four key muscles essential for lower body locomotion; biceps femoris long head(knee\\nflexion and hip extension), gluteus maximus(hip extension and external rotation), gluteus medius\\n(abduction and medial rotation of the hip), and rectus femoris(hip flexion and knee extension), each\\nfor the left and right body half. The kick is clearly executed with the left leg with rectus femoris\\nproviding the force for the swing in the second half of the motion and the other muscles of the\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 8, 'page_label': '9'}, page_content='left leg preparing it in the first half. During jumping jacks, gluteus mediusand rectus femorisare\\nactivated alternatingly for both legs. Predicted muscle activations closely follow the ground truth\\nfrom our dataset, with slight underestimation at the activation peaks. Similar estimation quality can\\nbe observed across the test set and we refer the reader to the appendix where we provide a larger\\nnumber of randomly selected results for qualitative analysis.\\n6 Discussion\\nWe believe that enhancing models through detailed muscle activation data aligned with human motion\\nis a worthwhile direction to explore in the future, which is now made possible by the presented MinT\\ndataset. The dataset offers a large amount of intricately simulated data, based on real human motions,\\nand utilizing bio-mechanically validated musculoskeletal models. By showing that neural models\\ncan learn to connect motion input to muscle activation sequences, we broaden the pathway towards\\nmodels which understand the nuanced interplay between motion and muscles.\\nSocietal ImpactWhile the dataset has a good balance in terms of gender distribution, ethnicity is not\\ndistributed equally, and some body-weight types are less represented, impacting the dataset diversity.\\nLimitations MinT is a simulation dataset, and despite careful design of our pipeline and rigorous data\\nanalysis, a synthetic-to-real domain gap remains inevitable. Researchers should be mindful of these\\nlimitations and consider their potential impact on real-world applications. Any models or analysis\\nbased on MinT require appropriate validation, ideally with real-world experiments.\\nOur simulations are a computationally intensive process. Given the potential for non-convergence\\nin complex movement data, we imposed an iteration limit, discarding samples which do not meet a\\npredefined error tolerance within this range. This potentially creates a category distribution shift in\\ncomparison to AMASS, since some motion categories might generally be harder to simulate.\\nFurthermore, the dataset is mostly restricted to motion types limited to foot-ground contact alone;\\nmotions with environment contact by other body parts or interactions with external objects were\\nmostly excluded due to missing information about such reaction forces. We included certain object-\\nrelated motions, such as lifting and throwing, as these motions are especially valuable for examining\\nback muscle activation. Since we miss information about object mass, we assume interaction with\\nvery small, lightweight objects of negligible weight in these cases.\\nMore extensive descriptions of these design decisions and preprocessing steps are provided in the\\nappendix, including details on runtime distribution and error handling, to offer transparency for\\nresearchers seeking to adapt or expand upon our approach.\\n7 Conclusion\\nThe quest to analyze human motion necessitates a critical component that has been notably absent:\\na comprehensive biomechanical dataset. Our contribution, the Muscles in Time (MinT) dataset,\\naddresses this gap by providing an unprecedented collection of synthetic muscle activation data.\\nThis dataset encompasses 402 distinct simulated muscle strains, all derived from authentic human\\nmovements, thus offering a vital resource for human motion research. Our methodology entails\\na scalable pipeline that utilizes cutting-edge musculoskeletal models to derive muscle activations\\nfrom recorded human motion sequences. The culmination of this process is the MinT dataset, which\\nalso contains 9.8 hours of time series data representing muscle activations. We demonstrate that\\nneural networks can effectively utilize this muscle activation data to discern patterns linking motion\\nto muscle activation. This represents a significant stride towards a deeper comprehension of human\\nmotion from a biomechanical standpoint. The MinT dataset enables the research community in\\nexploration of human motion and muscular dynamics through a data-centric approach. Our work not\\nonly enriches the field of biomechanical studies but also paves the way for future advancements in\\nunderstanding the complex interplay of muscles in human movement.\\nAcknowledgements This work has been supported by the Carl Zeiss Foundation through the JuBot\\nproject as well as by funding from the pilot program Core-Informatics of the Helmholtz Association\\n(HGF). The authors acknowledge support by the state of Baden-Württemberg through bwHPC.\\nExperiments were performed on the HoreKa supercomputer funded by the Ministry of Science,\\nResearch and the Arts Baden-Württemberg and by the Federal Ministry of Education and Research.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10'}, page_content='References\\n[1] Ahmad, T., Jin, L., Zhang, X., Lai, S., Tang, G., Lin, L.: Graph convolutional neural network\\nfor human action recognition: A comprehensive survey. TAI (2021)\\n[2] Bittner, M., Yang, W.T., Zhang, X., Seth, A., van Gemert, J., van der Helm, F.C.T.: Towards\\nsingle camera human 3d-kinematics 23(1), 341. https://doi.org/10.3390/s23010341, https:\\n//www.mdpi.com/1424-8220/23/1/341\\n[3] Bruno, A.G., Bouxsein, M.L., Anderson, D.E.: Development and validation of a muscu-\\nloskeletal model of the fully articulated thoracolumbar spine and rib cage 137(8), 081003.\\nhttps://doi.org/10.1115/1.4030408\\n[4] Caggiano, V ., Wang, H., Durandau, G., Sartori, M., Kumar, V .: Myosuite–a contact-rich\\nsimulation suite for musculoskeletal motor control. arXiv preprint arXiv:2205.13600 (2022)\\n[5] Camargo, J., Ramanathan, A., Flanagan, W., Young, A.: A comprehensive, open-source dataset\\nof lower limb biomechanics in multiple conditions of stairs, ramps, and level-ground ambulation\\nand transitions. Journal of Biomechanics 119, 110320 (2021)\\n[6] Chan, W., Saharia, C., Hinton, G.E., Norouzi, M., Jaitly, N.: Imputer: Sequence modelling via\\nimputation and dynamic programming. In: ICML (2020)\\n[7] Chen, Y ., Peng, K., Roitberg, A., Schneider, D., Zhang, J., Zheng, J., Liu, R., Chen, Y ., Yang, K.,\\nStiefelhagen, R.: Unveiling the hidden realm: Self-supervised skeleton-based action recognition\\nin occluded environments. arXiv preprint arXiv:2309.12029 (2023)\\n[8] Chen, Y ., Zhang, Z., Yuan, C., Li, B., Deng, Y ., Hu, W.: Channel-wise topology refinement\\ngraph convolution for skeleton-based action recognition. In: ICCV (2021)\\n[9] Cheng, K., Zhang, Y ., Cao, C., Shi, L., Cheng, J., Lu, H.: Decoupling gcn with dropgraph\\nmodule for skeleton-based action recognition. In: ECCV (2020)\\n[10] Chiquier, M., V ondrick, C.: Muscles in action. In: CVPR (2023)\\n[11] Chiu, C.C., Sainath, T.N., Wu, Y ., Prabhavalkar, R., Nguyen, P., Chen, Z., Kannan, A., Weiss,\\nR.J., Rao, K., Gonina, E., et al.: State-of-the-art speech recognition with sequence-to-sequence\\nmodels. In: ICASSP (2018)\\n[12] Christ, M., Braun, N., Neuffer, J., Kempa-Liehr, A.W.: Time series feature extraction on basis\\nof scalable hypothesis tests (tsfresh–a python package). Neurocomputing 307, 72–77 (2018)\\n[13] Colombo, P., Chapuis, E., Manica, M., Vignon, E., Varni, G., Clavel, C.: Guiding attention in\\nsequence-to-sequence models for dialogue act prediction. In: AAAI (2020)\\n[14] Dao, T., Gu, A.: Transformers are SSMs: Generalized models and efficient algorithms through\\nstructured state space duality. In: International Conference on Machine Learning (ICML) (2024)\\n[15] Delp, S.L., Anderson, F.C., Arnold, A.S., Loan, P., Habib, A., John, C.T., Guendelman,\\nE., Thelen, D.G.: OpenSim: Open-source software to create and analyze dynamic simula-\\ntions of movement 54(11), 1940–1950. https://doi.org/10.1109/TBME.2007.901024, https:\\n//ieeexplore.ieee.org/abstract/document/4352056\\n[16] Delp, S.L., Anderson, F.C., Arnold, A.S., Loan, P., Habib, A., John, C.T., Guendelman,\\nE., Thelen, D.G.: Opensim: open-source software to create and analyze dynamic simula-\\ntions of movement. IEEE transactions on bio-medical engineering 54(11), 1940–1950 (2007).\\nhttps://doi.org/10.1109/TBME.2007.901024\\n[17] Ding, K., Liang, A.J., Perozzi, B., Chen, T., Wang, R., Hong, L., Chi, E.H., Liu, H., Cheng, D.Z.:\\nHyperFormer: Learning expressive sparse feature representations via hypergraph transformer.\\nIn: SIGIR (2023)\\n[18] Dong, L., Xu, S., Xu, B.: Speech-transformer: a no-recurrence sequence-to-sequence model for\\nspeech recognition. In: ICASSP (2018)\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 10, 'page_label': '11'}, page_content='[19] Duan, H., Zhao, Y ., Chen, K., Lin, D., Dai, B.: Revisiting skeleton-based action recognition. In:\\nCVPR (2022)\\n[20] Falisse, A., Serrancoli, G., Dembia, C.L., Gillis, J., Groote, F.D.: Algorithmic differenti-\\nation improves the computational efficiency of OpenSim-based trajectory optimization of\\nhuman movement 14(10), e0217730. https://doi.org/10.1371/journal.pone.0217730, https:\\n//journals.plos.org/plosone/article?id=10.1371/journal.pone.0217730\\n[21] Feldotto, B., Soare, C., Knoll, A., Sriya, P., Astill, S., de Kamps, M., Chakrabarty, S.: Evaluating\\nmuscle synergies with emg data and physics simulation in the neurorobotics platform. Frontiers\\nin Neurorobotics (2022)\\n[22] Feng, L., Zhao, Y ., Zhao, W., Tang, J.: A comparative review of graph convolutional networks\\nfor human skeleton-based action recognition. Artificial Intelligence Review (2022)\\n[23] Feng, Y ., Xu, X., Liu, L.: Musclevae: Model-based controllers of muscle-actuated characters.\\nIn: SIGGRAPH Asia 2023 Conference Papers. pp. 1–11 (2023)\\n[24] Foo, L.G., Li, T., Rahmani, H., Ke, Q., Liu, J.: Unified pose sequence modeling. In: CVPR\\n(2023)\\n[25] Furmanek, M.P., Mangalam, M., Yarossi, M., Lockwood, K., Tunik, E.: A kinematic and emg\\ndataset of online adjustment of reach-to-grasp movements to visual perturbations. Scientific\\ndata 9(1), 23 (2022)\\n[26] Gehring, J., Auli, M., Grangier, D., Yarats, D., Dauphin, Y .N.: Convolutional sequence to\\nsequence learning. In: International conference on machine learning. pp. 1243–1252. PMLR\\n(2017)\\n[27] Ghorbani, S., Mahdaviani, K., Thaler, A., Kording, K., Cook, D., Blohm, G., Troje, N.: Movi:\\nA large multipurpose motion and video dataset. arxiv 2020. arXiv preprint arXiv:2003.01888\\n(2020)\\n[28] Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating diverse and natural\\n3d human motions from text. In: CVPR (2022)\\n[29] He, K., Zuo, C., Ma, C., Sui, Y .: Dynsyn: Dynamical synergistic representation for efficient\\nlearning and control in overactuated embodied systems. arXiv preprint arXiv:2407.11472 (2024)\\n[30] Hernández, Ó.G., Lopez-Castellanos, J.M., Jara, C.A., Garcia, G.J., Ubeda, A., Morell-Gimenez,\\nV ., Gomez-Donoso, F.: A kinematic, imaging and electromyography dataset for human muscular\\nmanipulability index prediction. Scientific Data (2023)\\n[31] Hochreiter, S.: Long short-term memory. Neural Computation MIT-Press (1997)\\n[32] Hrinchuk, O., Popova, M., Ginsburg, B.: Correction of automatic speech recognition with\\ntransformer sequence-to-sequence model. In: ICASSP (2020)\\n[33] Jaitly, N., Le, Q.V ., Vinyals, O., Sutskever, I., Sussillo, D., Bengio, S.: An online sequence-to-\\nsequence model using partial conditioning. NeurIPS (2016)\\n[34] Jarque-Bou, N.J., Vergara, M., Sancho-Bru, J.L., Gracia-Ibáñez, V ., Roda-Sales, A.: A calibrated\\ndatabase of kinematics and emg of the forearm and hand during activities of daily living.\\nScientific data 6(1), 270 (2019)\\n[35] Jiang, Y ., Van Wouwe, T., De Groote, F., Liu, C.K.: Synthesis of biologically realistic human\\nmotion using joint torque actuation. ACM Transactions On Graphics (TOG) 38(4), 1–12 (2019)\\n[36] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: A new representation of skeleton\\nsequences for 3d action recognition. In: ICCV (2017)\\n[37] Keneshloo, Y ., Shi, T., Ramakrishnan, N., Reddy, C.K.: Deep reinforcement learning for\\nsequence-to-sequence models. TNNLS (2019)\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 11, 'page_label': '12'}, page_content='[38] Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks.\\narXiv preprint arXiv:1609.02907 (2016)\\n[39] Kocabas, M., Athanasiou, N., Black, M.J.: Vibe: Video inference for human body pose and\\nshape estimation. In: The IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR) (June 2020)\\n[40] Lai, A.K., Arnold, A.S., Wakeling, J.M.: Why are antagonist muscles co-activated in my simu-\\nlation? A musculoskeletal model for analysing human locomotor tasks. Annals of Biomedical\\nEngineering 45, 2762–2774 (2017)\\n[41] Lee, J., Lee, M., Lee, D., Lee, S.: Hierarchically decomposed graph convolutional networks for\\nskeleton-based action recognition. arXiv preprint arXiv:2208.10741 (2022)\\n[42] Lencioni, T., Carpinella, I., Rabuffetti, M., Marzegan, A., Ferrarin, M.: Human kinematic, ki-\\nnetic and emg data during different walking and stair ascending and descending tasks. Scientific\\ndata 6(1), 309 (2019)\\n[43] Li, C., Zhang, Z., Lee, W.S., Lee, G.H.: Convolutional sequence to sequence model for human\\ndynamics. In: CVPR (2018)\\n[44] Liu, M., Liu, H., Chen, C.: Enhanced skeleton visualization for view invariant human action\\nrecognition. PR (2017)\\n[45] Ma, L., Zhao, Y ., Wang, B., Shen, F.: A multi-step sequence-to-sequence model with attention\\nlstm neural networks for industrial soft sensor application. IEEE Sensors Journal (2023)\\n[46] Maleševi´c, N., Olsson, A., Sager, P., Andersson, E., Cipriani, C., Controzzi, M., Björkman,\\nA., Antfolk, C.: A database of high-density surface electromyogram signals comprising 65\\nisometric hand gestures. Scientific Data 8(1), 63 (2021)\\n[47] Mandery, C., Terlemez, Ö., Do, M., Vahrenkamp, N., Asfour, T.: The kit whole-body human\\nmotion database. In: 2015 International Conference on Advanced Robotics (ICAR). pp. 329–336.\\nIEEE (2015)\\n[48] Mandery, C., Terlemez, O., Do, M., Vahrenkamp, N., Asfour, T.: Unifying representations and\\nlarge-scale whole-body motion databases for studying human motion. TRO (2016)\\n[49] Marinov, Z., Schneider, D., Roitberg, A., Stiefelhagen, R.: Multimodal generation of novel\\naction appearances for synthetic-to-real recognition of activities of daily living. In: 2022\\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 11320–\\n11327. IEEE (2022)\\n[50] Matran-Fernandez, A., Rodríguez Martínez, I.J., Poli, R., Cipriani, C., Citi, L.: Seeds, simulta-\\nneous recordings of high-density emg and finger joint angles during multiple hand movements.\\nScientific data 6(1), 186 (2019)\\n[51] Moreira, L., Figueiredo, J., Fonseca, P., Vilas-Boas, J.P., Santos, C.P.: Lower limb kinematic,\\nkinetic, and emg data from young healthy humans during walking at controlled speeds. Scientific\\ndata 8(1), 103 (2021)\\n[52] Neubig, G.: Neural machine translation and sequence-to-sequence models: A tutorial. arXiv\\npreprint arXiv:1703.01619 (2017)\\n[53] Orvieto, A., Smith, S.L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., De, S.: Resurrecting\\nrecurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349 (2023)\\n[54] Peng, K., Roitberg, A., Yang, K., Zhang, J., Stiefelhagen, R.: Delving deep into one-shot\\nskeleton-based action recognition with diverse occlusions. TMM (2023)\\n[55] Peng, K., Schneider, D., Roitberg, A., Yang, K., Zhang, J., Sarfraz, M.S., Stiefelhagen,\\nR.: Musclemap: Towards video-based activated muscle group estimation. arXiv preprint\\narXiv:2303.00952 (2023)\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13'}, page_content='[56] Peng, K., Yin, C., Zheng, J., Liu, R., Schneider, D., Zhang, J., Yang, K., Sarfraz, M.S., Stiefel-\\nhagen, R., Roitberg, A.: Navigating open set scenarios for skeleton-based action recognition. In:\\nAAAI (2024)\\n[57] Phan, H., Andreotti, F., Cooray, N., Chén, O.Y ., De V os, M.: Seqsleepnet: end-to-end hier-\\narchical recurrent neural network for sequence-to-sequence automatic sleep staging. TNSRE\\n(2019)\\n[58] Plizzari, C., Cannici, M., Matteucci, M.: Spatial temporal transformer network for skeleton-\\nbased action recognition. In: ICPRW (2021)\\n[59] Punnakkal, A.R., Chandrasekaran, A., Athanasiou, N., Quiros-Ramirez, A., Black, M.J.: BA-\\nBEL: Bodies, action and behavior with english labels. In: Proceedings IEEE/CVF Conf. on\\nComputer Vision and Pattern Recognition (CVPR). pp. 722–731 (Jun 2021)\\n[60] Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T.P.: Compressive transformers for long-\\nrange sequence modelling. ArXiv abs/1911.05507 (2019), https://api.semanticscholar.\\norg/CorpusID:207930593\\n[61] Rojas-Martínez, M., Serna, L.Y ., Jordanic, M., Marateb, H.R., Merletti, R., Mañanas, M.Á.:\\nHigh-density surface electromyography signals during isometric contractions of elbow muscles\\nof healthy humans. Scientific data 7(1), 397 (2020)\\n[62] Sarfraz, M.S., Sharma, V ., Stiefelhagen, R.: Efficient parameter-free clustering using first\\nneighbor relations. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR). pp. 8934–8943 (2019)\\n[63] Sarfraz, S., Koulakis, M., Seibold, C., Stiefelhagen, R.: Hierarchical nearest neighbor graph\\nembedding for efficient dimensionality reduction. In: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition. pp. 336–345 (2022)\\n[64] Schneider, D., Keller, M., Zhong, Z., Peng, K., Roitberg, A., Beyerer, J., Stiefelhagen, R.:\\nSynthact: Towards generalizable human action recognition based on synthetic data. In: ICRA\\n(2024)\\n[65] Schreiber, C., Moissenet, F.: A multimodal dataset of human gait at different walking speeds\\nestablished on injury-free adult participants. Scientific data 6(1), 111 (2019)\\n[66] Seth, A., Hicks, J.L., Uchida, T.K., Habib, A., Dembia, C.L., Dunne, J.J., Ong, C.F., DeMers,\\nM.S., Rajagopal, A., Millard, M., Hamner, S.R., Arnold, E.M., Yong, J.R., Lakshmikanth, S.K.,\\nSherman, M.A., Ku, J.P., Delp, S.L.: Opensim: Simulating musculoskeletal dynamics and\\nneuromuscular control to study human and animal movement. PLoS computational biology\\n14(7), e1006223 (2018). https://doi.org/10.1371/journal.pcbi.1006223\\n[67] Shao, L., Gouws, S., Britz, D., Goldie, A., Strope, B., Kurzweil, R.: Generating high-quality\\nand informative conversation responses with sequence-to-sequence models. arXiv preprint\\narXiv:1701.03185 (2017)\\n[68] Shi, L., Zhang, Y ., Cheng, J., Lu, H.: Two-stream adaptive graph convolutional networks for\\nskeleton-based action recognition. In: CVPR (2019)\\n[69] Shi, L., Zhang, Y ., Cheng, J., Lu, H.: Decoupled spatial-temporal attention network for skeleton-\\nbased action-gesture recognition. In: ACCV (2020)\\n[70] Uhlrich, S.D., Falisse, A., Kidzi´nski, Ł., Muccini, J., Ko, M., Chaudhari, A.S., Hicks, J.L., Delp,\\nS.L.: Opencap: Human movement dynamics from smartphone videos. PLoS computational\\nbiology 19(10), e1011462 (2023)\\n[71] Vaswani, A.: Attention is all you need. Advances in Neural Information Processing Systems\\n(2017)\\n[72] Wei, Y ., Peng, K., Roitberg, A., Zhang, J., Zheng, J., Liu, R., Chen, Y ., Yang, K., Stiefelhagen,\\nR.: Elevating skeleton-based action recognition with efficient multi-modality self-supervision.\\nIn: ICASSP (2024)\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14'}, page_content='[73] Xin, W., Liu, R., Liu, Y ., Chen, Y ., Yu, W., Miao, Q.: Transformer for skeleton-based action\\nrecognition: A review of recent advances. Neurocomputing (2023)\\n[74] Xu, Y ., Peng, K., Wen, D., Liu, R., Zheng, J., Chen, Y ., Zhang, J., Roitberg, A., Yang, K.,\\nStiefelhagen, R.: Skeleton-based human action recognition with noisy labels. arXiv preprint\\narXiv:2403.09975 (2024)\\n[75] Yan, H., Liu, Y ., Wei, Y ., Li, Z., Li, G., Lin, L.: Skeletonmae: graph-based masked autoencoder\\nfor skeleton sequence pre-training. In: ICCV (2023)\\n[76] Yan, S., Xiong, Y ., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based\\naction recognition. In: AAAI (2018)\\n[77] Ye, F., Pu, S., Zhong, Q., Li, C., Xie, D., Tang, H.: Dynamic gcn: Context-enriched topology\\nlearning for skeleton-based action recognition. In: MM (2020)\\n[78] Zhang, J., Zhang, Y ., Cun, X., Huang, S., Zhang, Y ., Zhao, H., Lu, H., Shen, X.: T2m-gpt:\\nGenerating human motion from textual descriptions with discrete representations. In: CVPR\\n(2023)\\n[79] Zhang, Q.: Experimental data of semg, us imaging, grf, and markers for walking on treadmill\\nacross multiple speeds (2021). https://doi.org/10.21227/7beh-f093, https://dx.doi.org/10.\\n21227/7beh-f093\\n[80] Zhong, Z., Schneider, D., V oit, M., Stiefelhagen, R., Beyerer, J.: Anticipative feature fusion\\ntransformer for multi-modal action anticipation. In: Proceedings of the IEEE/CVF Winter\\nConference on Applications of Computer Vision. pp. 6068–6077 (2023)\\n[81] Zhou, Y ., Li, C., Cheng, Z.Q., Geng, Y ., Xie, X., Keuper, M.: Hypergraph transformer for\\nskeleton-based action recognition. arXiv preprint arXiv:2211.09590 (2022)\\n[82] Zimmer, P., Appell, H.J.: Funktionelle Anatomie: Grundlagen sportlicher Leistung und Bewe-\\ngung. Springer Berlin Heidelberg, Berlin, Heidelberg (2021). https://doi.org/10.1007/978-3-\\n662-61482-2\\n[83] Zuo, C., He, K., Shao, J., Sui, Y .: Self model for embodied intelligence: Modeling full-body\\nhuman musculoskeletal system and locomotion control with hierarchical low-dimensional\\nrepresentation. In: 2024 IEEE International Conference on Robotics and Automation (ICRA).\\npp. 13062–13069. IEEE (2024)\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15'}, page_content='A Appendix\\nA.1 Dataset Information\\nThe main entry point to interact with our work is our project page under\\nhttps://simplexsigil.github.io/mint.\\nLicense The MINT dataset is built on top of the KIT Whole-Body Human Motion Database,\\nBMLmovi, BMLrub, the EyesJapan dataset and TotalCapture. We make use of AMASS to map from\\nthe motions of these original datasets to virtual marker positions in OpenSim.\\nAll of these datasets allow usage of their data for non-commercial scientific research:\\n• The license of AMASS can be found under https://amass.is.tue.mpg.de/license.html\\n• The License of BMLmovi and BMLrub can be found under\\nhttps://www.biomotionlab.ca/movi/\\n• The KIT Whole-Body Human Motion Database can be used upon citation of the original\\nwork as explained here https://download.is.tue.mpg.de/amass/licences/kit.html\\n• The license for the EyesJapan dataset can be found under\\nhttp://mocapdata.com/Terms_of_Use.html\\n• The license for the Total Capture dataset can be found under\\nhttps://cvssp.org/data/totalcapture/\\nThe Muscles in Time dataset is published under a CC BY-NC 4.0 license as defined under\\nhttps://creativecommons.org/licenses/by-nc/4.0/. Researchers making use of this dataset must also\\nagree to the licenses mentioned above which can add additional restrictions depending on the individ-\\nual sub-dataset.\\nOur data generation pipeline is licensed under Apache License Version 2.0 as defined under\\nhttps://apache.org/licenses/LICENSE-2.0.\\nCode for training our muscle activation estimation networks is licensed under the MIT license as\\ndefined under https://opensource.org/license/mit.\\nAuthor statement The authors of this work bear the responsibility for publishing the MinT dataset\\nand related code and data.\\nData structure The structure of the provided MinT data is intentionally kept simple. All data is\\nsaved in CSV files or pandas DataFrames stored in pickle files. In Listing 1 we display how data for\\nan individual sample can be loaded with minimal dependencies ( joblib and pandas). We provide\\nmuscle activations in a range of [0, 1], ground reaction forces and effective muscle forces. Data\\nis provided with 50 fps, each dataframe is indexed by fractional timestamps. Columns are named\\nmeaningfully, the first 80 muscles belong to the lower body model, the following 322 muscles belong\\nto the upper body model. The first and last 0.14 seconds are cut off since the muscle activation\\nanalysis is unstable towards the beginning and end of data. Since the data is generated in chunks of\\n1.4 seconds and muscle activation analysis can fail to succeed due to various factors, the provided\\ndata may contain gaps identified by missing data for certain time ranges.\\nThe musint package To further facilitate the usage of the MinT dataset, we provide the musint\\npackage, a Python package that allows data to be loaded into a predefined torch dataset and allows\\nsimplified cross-referencing with BABEL dataset labels. Additionally, it includes functionality for\\nsampling a sub-window of the data at a given framerate as well as identifying and handling any gaps\\nin the data. A short example on the musint package usage is displayed in Listing 2.\\nThe musint package can be installed via pip install musint. Additional insight can be found on\\nthe musint github page where we also provide a Jupyter notebook for displaying the data as well as\\nadditional information on muscle subsets:\\nhttps://github.com/simplexsigil/MusclesInTime\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16'}, page_content='1 >>> # First download and extract the dataset .\\n2 >>> # Example for sample\\n3 >>> #’ BMLmovi / BMLmovi / Subject_11_F_MoSh / Subject_11_F_10_poses ’\\n4 >>> import joblib\\n5 >>> joblib . load (\" muscle_activations . pkl \")\\n6 LU_addbrev_l ... TL_TR4_r TL_TR5_r\\n7 0.14 0.016 ... 0.003 0.061\\n8 0.16 0.028 ... 0.005 0.070\\n9 0.18 0.033 ... 0.002 0.080\\n10 ... ... ... ... ...\\n11 3.74 0.024 ... 0.020 0.028\\n12 3.76 0.016 ... 0.009 0.004\\n13 3.78 0.011 ... 0.003 0.000\\n14\\n15 [183 rows x 402 columns ]\\n16\\n17 >>> joblib . load (\" grf . pkl \")\\n18 ground_force_right_vx ... ground_torque_left_z\\n19 0.14 15.962 ... 0.0\\n20 0.16 10.596 ... 0.0\\n21 0.18 3.422 ... 0.0\\n22 ... ... ... ...\\n23 3.72 20.337 ... 0.0\\n24 3.74 21.572 ... 0.0\\n25 3.76 22.546 ... 0.0\\n26\\n27 [182 rows x 18 columns ]\\n28\\n29 >>> joblib . load (\" muscle_forces . pkl \")\\n30 LU_addbrev_l ... TL_TR4_r TL_TR5_r\\n31 0.14 8.430 ... 0.153 11.652\\n32 0.16 15.345 ... 0.283 13.240\\n33 0.18 19.127 ... 0.143 15.240\\n34 ... ... ... ... ...\\n35 3.72 14.437 ... 1.320 3.661\\n36 3.74 13.993 ... 1.270 5.330\\n37 3.76 9.346 ... 0.577 0.847\\n38\\n39 [182 rows x 402 columns ]\\nListing 1: Simplified loading of MinT samples with joblib and pandas.\\nA.2 Additional Statistics and Information\\nIn Figure 9 we provide additional information on the data analyzed provided with Muscles in Time.\\nTotal Capture makes up a small part of the dataset with exceptionally long sequences. The Eyes Japan\\nDataset provides the largest contribution with 3.2h of analyzed recordings.\\nIn Tables 3 and 4, we list larger muscle groups in the lower and upper body model as well as their\\nfunction for human motion. Muscle groups or larger muscles can be represented by multiple simulated\\nmuscles, e.g. since such muscles are attached to multiple muscle locations or exert forces in varying\\ndirections. The Gluteus Mediusmuscle is an example with three simulated activations on each side\\nof the body.\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 16, 'page_label': '17'}, page_content='1 >>> # First download and extract the dataset .\\n2 >>> import os\\n3 >>> from musint . datasets . mint_dataset import MintDataset\\n4\\n5 >>> md = MintDataset (os. path . expandvars (\" $MINT_ROOT \"))\\n6\\n7 >>> md. by_path (\" TotalCapture / TotalCapture /s1/ acting2_poses \")\\n8 MintData ( path_id =’s1/ acting2 ’, babel_sid =12906 , dataset =’\\nTotalCapture ’, amass_dur =61.7 , num_frames =1114 , fps =50.0 ,\\nanalysed_dur =22.26 , analysed_percentage =0.36 , data_path =’\\nTotalCapture / TotalCapture /s1/ acting2_poses ’, weight =72.1 ,\\nheight =169.2 , subject =’s1 ’, sequence =’ acting2_poses ’,\\ngender =’male ’, has_gap = False , dtype = object ))\\n9\\n10 >>> md. by_path (\" TotalCapture / TotalCapture /s1/ acting2_poses \").\\nget_muscle_activations ( time_window =(0.3 ,1.) ,\\ntarget_frame_count = int (0.7*20.) )\\n11 LU_addbrev_l ... TL_TR4_r TL_TR5_r\\n12 0.30 0.094 ... 0.000 0.020\\n13 0.36 0.094 ... 0.003 0.042\\n14 0.40 0.091 ... 0.000 0.027\\n15 ... ... ... ... ...\\n16 0.90 0.093 ... 0.000 0.008\\n17 0.94 0.093 ... 0.000 0.000\\n18 1.00 0.094 ... 0.001 0.009\\n19\\n20 [14 rows x 402 columns ]\\nListing 2: Loading the MinT dataset with the python musint package.\\nA.3 Design Choices and More Detailed Limitations\\nThe muscle-driven simulation, based on the approach by Falisseet al. [20], aims to ensure that muscle\\nand skeletal dynamics align closely with given kinematic data while minimizing muscle effort. This\\nprocess involves finding a solution within the problem space that satisfies an error tolerance and the\\nnumber of collocation points, which depend on the dynamics of the kinematic data. Collocation\\npoints are used to discretize the continuous kinematic and dynamic equations into a finite set of points,\\nmaking the optimization problem computationally feasible. To mitigate the risk of non-convergent\\nor non-meaningful solutions, we implemented safeguards by restricting the deviation between the\\nkinematic information before and after the optimization problem converges.\\nGiven the computational complexity, we decided to use 50 collocation points per second and an\\nerror tolerance of 10−3. On an Intel Xeon Gold 6230 with 96 GB RAM, processing 6 subsequences\\nof 1.68 seconds (including 0.14 second buffers at start and end) in parallel took approximately a\\nmedian time of 45 minutes. Figure 10 displays a distribution of sample-wise runtimes in a violin\\nplot. Non-converging samples tend to have higher runtimes and can be found on the long tail on\\nthe right. To manage the impact of unsuccessful simulations on the overall runtime, we limited the\\noptimization problem to 2500 iterations and discard a sample if the optimization does not fall within\\nerror tolerance after this time. The AMASS sequences were divided into 1.4-second segments to\\nmitigate a nonlinearly increasing runtime associated with longer motion sequences. After simulation,\\nthese segments were recombined into the original sequences, with muscle values smoothed at the\\nconnection points to ensure seamless transitions.\\nA challenge arose from minor variable distances between the AMASS body model and the ground,\\nsince the contact spheres provided by the OpenCap simulation are susceptible to changes in foot-\\nground distance. To provide similar foot-ground distances over all AMASS subjects, our pipeline\\nautomatically offsets the AMASS model depending on the lowest body marker over the time of the\\nsequence.\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18'}, page_content='Figure 7: Virtual marker placement for transferring motions to OpenSim, enlarged from Figure 2.\\nFigure 8: Lower body and upper body model, enlarged from Figure 2.\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 18, 'page_label': '19'}, page_content='Eyes Japan\\n3.2 h\\nKIT\\n1.7h\\nTotal Capture\\n0.3h\\nBML Movie\\n1.7h\\nBML Rub\\n2.8h\\n9.8h in Total\\nBML Movie BML Rub Eyes\\nJapan\\nKIT Total\\nCapture\\nAverage Number of Labels per Sequence\\nAverage Time per Sequence\\nBML Movie BML Rub Eyes\\nJapan\\nKIT Total\\nCapture\\nFigure 9: Average number of labels per sequence, composition of sub datasets and average sequence\\nlength.\\nFigure 10: Analysis runtime distribution of the optimal trajectory problem described by Falisse et\\nal. [20]. Subset of 10k runs.\\nMapping AMASS motions to OpenSim models presented difficulties due to the numerous degrees\\nof freedom in the Thoracolumbar model, complicating kinematic analysis. To safeguard the ver-\\ntebral joints against aberrant movements, we constrained the range of motion for each vertebra,\\napproximating the natural degrees of freedom in the vertebrae joints.\\nThe MinT dataset was restricted to motions involving foot-ground contact only. Motions involving\\nground contact of other body parts or involving objects were excluded, except for motions that\\nincluded throwing and lifting, which are particularly relevant for analyzing back muscle activation. In\\nthese cases, we assumed the objects’ mass to be negligible, as the AMASS dataset does not provide\\nthis information.\\nA.4 Results for Additional Muscle Subsets\\nTo facilitate comparability to real world recordings as well as to other datasets, we define two muscle\\nsubsets of the lower body model, containing either 16 or eight of the most important lower body\\nmuscles for human locomotion. The subset LAI_ARNOLD_LOWER_BODY_16 contains left gluteus\\nmedius 1, left adductor magnus ischial part, left gluteus maximus 2, left iliacus, left rectus femoris,\\nleft biceps femoris long head, left gastrocnemius medial head, left tibialis anterior, right gluteus\\nmedius 1, right adductor magnus ischial part, right gluteus maximus 2, right iliacus, right rectus\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20'}, page_content='Table 3: List of muscle groups modelled in the model by Lai et al. [40], which are analysed in the\\npresented approach, and their functions [82].\\nMuscle Function\\nGluteus Maximus Extension and rotation of the hip.\\nGluteus Medius Abduction and rotation of the thigh.\\nGluteus Minimus Abduction and rotation of the thigh.\\nAdductor Brevis Adduction, flexion, and rotation of the thigh.\\nAdductor Longus Adduction and flexion of the thigh.\\nAdductor Magnus Adduction, flexion and rotation of the thigh.\\nGracilis Adduction, flexion and rotation of the thigh.\\nSemitendinosus Flexion and rotation of the knee, as well as extension of the\\nhip.\\nSemimembranosus Flexion and rotation of the knee, as well as extension of the\\nhip.\\nTensor Fasciae Latae Abduction and rotation of the thigh, as well stabilisation of\\nthe pelvis.\\nPiriformis Rotation and extension of the thigh and abduction of thigh.\\nSartorius Flexion, abduction, and rotation of the hip and flexion of the\\nknee.\\nIliacus Flexion of the hip.\\nPsoas Flexion and rotation of the hip.\\nRectus Femoris Flexion of hip and extension of knee.\\nBiceps Femoris Flexion of knee and extension of hip.\\nMedial Gastrocnemius Flexion of foot and flexion of knee.\\nLateral Gastrocnemius Plantar flexion and knee flexion.\\nTibialis Anterior Dorsiflexion and inversion of the foot.\\nVastus Extension of the knee.\\nExtensor Digitorum Longus Extension of toes and dorsiflexion of the foot.\\nExtensor Hallucis Longus Extension of the big toe and dorsiflexion of the foot.\\nFlexor Digitorum Longus Flexion of toes, as well as plantar flexion and inversion of\\nthe foot.\\nFlexor Hallucis Longus Flexion of toes, as well as plantar flexion and inversion of\\nthe foot.\\nPeroneus (Fibularis) Plantar flexion and eversion of the foot.\\nSoleus Plantar flexion of the foot.\\nfemoris, right biceps femoris long head, right gastrocnemius medial headand right tibialis anterior\\nwhile the muscle subset LAI_ARNOLD_LOWER_BODY_8 contains left gluteus medius 1, left gluteus\\nmaximus 2, left rectus femoris, left biceps femoris long head, right gluteus medius 1, right gluteus\\nmaximus 2, right rectus femorisand right biceps femoris long head. These subsets are also defined\\nwithin the musint package.\\nIn Table 5 we list the results of our 16 layer transformer model on these subsets.\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21'}, page_content='Table 4: List of muscle groups modelled in the model by Bruno et al. [3], which are analysed in the\\npresented approach, and their functions [82].\\nMuscle Function\\nLongissimus Extension and rotation of the vertebrae.\\nIliocostalis Extension and flexion of the neck.\\nSemispinalis Extension and rotation of the vertebrae.\\nSplenius Extension and rotation of the vertebrae.\\nSternocleidomastoid Flexion and rotation of the head.\\nScalenus Elevation of ribs and flexion of the neck.\\nLongus Colli Flexion of the neck and stabilisation of the cervical spine.\\nLevator Scapulae Elevation and adduction of the scapula.\\nQuadratus Lumborum Flexion the vertebral column.\\nMultifidus Stabilisation cervical vertebrae.\\nRectus Abdominis Flexion of the lumbar spine.\\nExternal Oblique Flexion and rotation of the trunk.\\nInternal Oblique Flexion and rotation of the trunk.\\nTransversus Abdominus Stabilisation of the trunk.\\nTable 5: Human motion-to-muscle activation prediction results for the lower body model.\\nMotion All muscles Lower Body Subset 16 Subset 8\\nRMSE↓ PCC↑ SMAPE↓ RMSE↓ PCC↑ SMAPE↓ RMSE↓ PCC↑ SMAPE↓ RMSE↓ PCC↑ SMAPE↓\\noverall 0.036 0.55 95.3 0.048 0.54 45.1 0.066 0.56 47.7 0.060 0.56 45.0\\njump 0.052 0.64 100.7 0.051 0.71 52.3 0.059 0.71 55.5 0.056 0.70 54.2\\nkick 0.046 0.64 102.8 0.053 0.62 54.8 0.068 0.63 57.0 0.067 0.67 57.4\\nstand 0.033 0.56 97.5 0.046 0.58 45.0 0.062 0.61 47.5 0.052 0.59 43.6\\nwalk 0.026 0.65 90.7 0.044 0.77 42.4 0.060 0.77 43.3 0.057 0.77 43.4\\njog 0.033 0.71 99.0 0.046 0.71 51.1 0.063 0.75 51.8 0.062 0.71 52.7\\ndance 0.041 0.60 109.2 0.057 0.65 58.5 0.073 0.66 59.6 0.072 0.67 59.5\\nA.5 Training on Muscles in Action\\nWe evaluate the generalizability of MinT by fine-tuning our 16-layer transformer architecture exclu-\\nsively on the first and last transformer block and comparing the results with full training from scratch\\non Muscles in Action [10]. The motions in MIA were obtained with VIBE [39], a 3D pose estimation\\nmethod performed on 2D images. The resulting motions are very noisy in contrast to the motions in\\nAMASS which are the result of motion capture, inducing a significant domain gap. Table 6 shows\\nour results. We find that limiting our training to the first and last transformer block results in very\\nsimilar RMSE values compared to full fine-tuning, while PCC and SMAPE clearly displays a small\\nbut significant advantage of the full fine-tuning strategy. Still, finetuning the first and last layer only\\naffects some 8% of all trainable weights, and we see this as an indication for the transferability of the\\nknowledge obtained by training on MinT.\\nA.6 Additional Qualitative Examples for MinT\\nFigure 6, in the main paper, lists two qualitative examples to display the muscle activation estimation\\nquality of our best model. Additionally, Figures 11 to 17 show 48 randomly chosen samples from the\\ntest set.\\n21'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 21, 'page_label': '22'}, page_content='Table 6: Human motion-to-muscle activation prediction results on Muscles in Action [10].\\nMotion Full Fine-tuning First and last layer\\nRMSE↓ PCC↑ SMAPE↓ RMSE↓ PCC↑ SMAPE↓\\nOverall 15.11 0.27 37.0 15.15 0.21 41.6\\nElbowPunch 15.66 0.25 43.6 15.48 0.19 48.8\\nFrontKick 8.49 0.19 34.5 8.20 0.14 41.0\\nFrontPunch 8.47 0.38 29.8 8.22 0.36 36.3\\nHighKick 13.09 0.35 37.0 12.94 0.29 39.7\\nHookPunch 13.18 0.32 37.1 13.28 0.28 44.6\\nJumpingJack 13.79 0.27 28.5 13.42 0.23 29.5\\nKneeKick 12.32 0.25 37.3 12.26 0.16 43.0\\nLegBack 11.70 0.32 37.3 11.91 0.18 44.4\\nLegCross 13.89 0.17 42.7 13.84 0.11 48.9\\nRonddeJambe 15.81 0.20 39.5 15.50 0.17 42.6\\nRunning 7.53 0.30 26.3 7.25 0.24 27.4\\nShuffle 9.79 0.21 28.0 9.56 0.13 30.5\\nSideLunges 26.13 0.29 45.9 26.66 0.22 51.7\\nSlowSkater 20.15 0.26 42.1 20.81 0.19 47.2\\nSquat 22.68 0.26 44.9 22.76 0.21 48.2\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 22, 'page_label': '23'}, page_content='Figure 11: Muscle activation estimation with our 16 layer transformer model.\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24'}, page_content='Figure 12: Muscle activation estimation with our 16 layer transformer model.\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25'}, page_content='Figure 13: Muscle activation estimation with our 16 layer transformer model.\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26'}, page_content='Figure 14: Muscle activation estimation with our 16 layer transformer model.\\n26'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 26, 'page_label': '27'}, page_content='Figure 15: Muscle activation estimation with our 16 layer transformer model.\\n27'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28'}, page_content='Figure 16: Muscle activation estimation with our 16 layer transformer model.\\n28'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-04T01:05:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-04T01:05:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/MuscleInTime.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29'}, page_content='Figure 17: Muscle activation estimation with our 16 layer transformer model.\\n29')]\n"
     ]
    }
   ],
   "source": [
    "print(all_pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab799e4d",
   "metadata": {},
   "source": [
    "## Setting up the Model and System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76668185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        '''\n",
    "        You are a research assistant specialized in analyzing scientific papers.\n",
    "\n",
    "        You must only use the retrieved context to answer. If the answer cannot be fully supported by the provided context, say “Not enough information in the retrieved documents.”\n",
    "\n",
    "        When answering:\n",
    "        -Cite passages using the citation format used by the RAG system (e.g., [source_3]).\n",
    "        -Do not hallucinate definitions, math, or claims not present in the retrieved text.\n",
    "        -Use clear, structured, academic language.\n",
    "        -When comparing or summarizing, reference which retrieved chunks support each point.\n",
    "        -If the user asks for opinions, base them strictly on retrieved evidence.\n",
    "\n",
    "        Your task: provide precise, context-grounded, verifiable answers.\n",
    "        '''\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a19ef2",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "454dc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is smpl model, what does it do?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The SMPL (Skinned Multi-Person Linear) model is a statistical body model that represents a person's shape and pose [source_1, source_13]. It is an additive model in vertex space [source_1]. The model is a function of joint angles and shape parameters [source_2].\n",
      "\n",
      "The SMPL model learns blend shapes from a large set of training meshes to represent various poses [source_1]. It learns a simplified function that relates pose to blend-shape weights, which is linear in the elements of part rotation matrices [source_1]. This allows the model to generalize to different poses and is efficient for animation in game engines [source_1]. The model can be animated using conventional animation methods in software like Maya and Unity [source_4].\n"
     ]
    }
   ],
   "source": [
    "query = \"what is smpl model, what does it do?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcltech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
